{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Instruction Fine-tuning: Llama-Guard-3-8B for Polarization Detection\n",
    "\n",
    "This notebook demonstrates instruction fine-tuning of the `meta-llama/Llama-Guard-3-8B` model on the polarization detection dataset.\n",
    "\n",
    "## Approach\n",
    "- Use QLoRA (Quantized Low-Rank Adaptation) for efficient fine-tuning\n",
    "- Format data as instruction-following conversations\n",
    "- Train on multilingual polarization detection task\n",
    "- Evaluate on held-out test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-deps",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-Guard-3-8B\"\n",
    "OUTPUT_DIR = \"./llama-guard-3-8b-polarization\"\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = \"../../subtask1/train\"\n",
    "DEV_DATA_PATH = \"../../subtask1/dev\"\n",
    "\n",
    "# Training hyperparameters\n",
    "MAX_SEQ_LENGTH = 512\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_RATIO = 0.03\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(split_dir):\n",
    "    \"\"\"Load all CSV files from a directory and combine them.\"\"\"\n",
    "    dfs = []\n",
    "    for file in os.listdir(split_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            lang = file.replace(\".csv\", \"\")\n",
    "            df = pd.read_csv(os.path.join(split_dir, file))\n",
    "            df[\"lang\"] = lang\n",
    "            dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "# Load training and dev data\n",
    "train_df = load_split(TRAIN_DATA_PATH)\n",
    "dev_df = load_split(DEV_DATA_PATH)\n",
    "\n",
    "print(f\"Train size: {train_df.shape}\")\n",
    "print(f\"Dev size: {dev_df.shape}\")\n",
    "print(f\"\\nLanguages: {train_df['lang'].nunique()}\")\n",
    "print(\"\\nPolarization distribution (train):\")\n",
    "print(train_df[\"polarization\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stratified train/val/test splits\n",
    "train_df[\"lang_label\"] = (\n",
    "    train_df[\"lang\"].astype(str) + \"_\" + train_df[\"polarization\"].astype(str)\n",
    ")\n",
    "\n",
    "train_data, temp_data = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.10,\n",
    "    stratify=train_df[\"lang_label\"],\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data,\n",
    "    test_size=0.50,\n",
    "    stratify=temp_data[\"lang_label\"],\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_data)}\")\n",
    "print(f\"Val: {len(val_data)}\")\n",
    "print(f\"Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "format-header",
   "metadata": {},
   "source": [
    "## 5. Format Data as Instructions\n",
    "\n",
    "We'll format the data using a chat template suitable for Llama-Guard-3-8B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-instructions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(text, lang, label=None):\n",
    "    \"\"\"Format a single example as an instruction-following conversation.\"\"\"\n",
    "    system_prompt = \"\"\"You are an expert content moderator specializing in detecting polarized content in social media posts.\n",
    "\n",
    "Polarized content includes:\n",
    "- Hate speech\n",
    "- Toxicity\n",
    "- Misogyny or gender-based violence\n",
    "- Sarcastic or offensive speech\n",
    "- Strong us-vs-them divisions\n",
    "- Extreme opinions that create hostility between groups\n",
    "\n",
    "Your task is to classify whether the given text contains polarized content. Respond with only 'Yes' or 'No'.\"\"\"\n",
    "\n",
    "    user_message = f\"\"\"Language: {lang}\n",
    "Text: {text}\n",
    "\n",
    "Does this text contain polarized content? Answer with only 'Yes' or 'No'.\"\"\"\n",
    "\n",
    "    if label is not None:\n",
    "        assistant_message = \"Yes\" if label == 1 else \"No\"\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_message},\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "# Format the datasets\n",
    "def prepare_dataset(df):\n",
    "    formatted_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        formatted_data.append(\n",
    "            format_instruction(row[\"text\"], row[\"lang\"], row[\"polarization\"])\n",
    "        )\n",
    "    return Dataset.from_list(formatted_data)\n",
    "\n",
    "\n",
    "train_dataset = prepare_dataset(train_data)\n",
    "val_dataset = prepare_dataset(val_data)\n",
    "test_dataset = prepare_dataset(test_data)\n",
    "\n",
    "print(\"Dataset prepared!\")\n",
    "print(\"\\nExample formatted instruction:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 6. Load Model and Tokenizer with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix for fp16 training\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Configure 4-bit quantization for QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lora-header",
   "metadata": {},
   "source": [
    "## 7. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lora-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "## 8. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-args",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    tf32=True,  # Use TF32 for faster training on Ampere GPUs\n",
    "    max_grad_norm=0.3,\n",
    "    weight_decay=0.001,\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainer-header",
   "metadata": {},
   "source": [
    "## 9. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formatting-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    \"\"\"Format examples using the chat template.\"\"\"\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"messages\"])):\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            example[\"messages\"][i], tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SFTTrainer (Supervised Fine-Tuning Trainer)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,  # Don't pack multiple examples into one sequence\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## 10. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 11. Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## 12. Evaluation and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_polarization(text, lang, model, tokenizer):\n",
    "    \"\"\"Predict polarization for a single text.\"\"\"\n",
    "    # Format the input\n",
    "    formatted = format_instruction(text, lang)\n",
    "\n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        formatted[\"messages\"], tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract answer (last part after the prompt)\n",
    "    answer = response.split(\"assistant\")[-1].strip().lower()\n",
    "\n",
    "    # Parse answer\n",
    "    if \"yes\" in answer:\n",
    "        return 1\n",
    "    elif \"no\" in answer:\n",
    "        return 0\n",
    "    else:\n",
    "        print(f\"Unclear response: {answer}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single example\n",
    "test_text = \"I hate the way you talk, I hate the way you walk\"\n",
    "test_lang = \"eng\"\n",
    "\n",
    "prediction = predict_polarization(test_text, test_lang, model, tokenizer)\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Language: {test_lang}\")\n",
    "print(f\"Prediction: {'Polarized' if prediction == 1 else 'Not Polarized'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate on test set (sample for speed)\n",
    "test_sample = test_data.sample(n=min(500, len(test_data)), random_state=42)\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "print(f\"Evaluating on {len(test_sample)} test examples...\")\n",
    "for idx, row in test_sample.iterrows():\n",
    "    pred = predict_polarization(row[\"text\"], row[\"lang\"], model, tokenizer)\n",
    "    predictions.append(pred)\n",
    "    true_labels.append(row[\"polarization\"])\n",
    "\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(test_sample)} examples\")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions, average=\"binary\")\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(\"Test Set Evaluation Results\")\n",
    "print(f\"{'=' * 50}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        true_labels, predictions, target_names=[\"Not Polarized\", \"Polarized\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-dev-header",
   "metadata": {},
   "source": [
    "## 13. Generate Predictions for Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict-dev",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generate predictions for the entire dev set\n",
    "dev_predictions = []\n",
    "\n",
    "print(f\"Generating predictions for {len(dev_df)} dev examples...\")\n",
    "for idx, row in dev_df.iterrows():\n",
    "    pred = predict_polarization(row[\"text\"], row[\"lang\"], model, tokenizer)\n",
    "    dev_predictions.append({\"id\": row[\"id\"], \"polarization\": pred})\n",
    "\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(dev_df)} examples\")\n",
    "\n",
    "# Save predictions\n",
    "pred_df = pd.DataFrame(dev_predictions)\n",
    "pred_df.to_csv(\"dev_predictions.csv\", index=False)\n",
    "print(\"\\nPredictions saved to dev_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading and preparing multilingual polarization detection data\n",
    "2. Formatting data as instruction-following conversations\n",
    "3. Fine-tuning Llama-Guard-3-8B using QLoRA for efficiency\n",
    "4. Evaluating the fine-tuned model on test data\n",
    "5. Generating predictions for the dev set\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different instruction formats\n",
    "- Try different LoRA configurations (r, alpha, target modules)\n",
    "- Adjust training hyperparameters (learning rate, batch size, epochs)\n",
    "- Implement few-shot prompting in the instruction format\n",
    "- Compare performance with encoder-only models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
