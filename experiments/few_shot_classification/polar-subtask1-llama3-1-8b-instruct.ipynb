{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d93033b8-88c1-46ac-979a-433112df83db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uv\n",
      "  Downloading uv-0.9.21-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading uv-0.9.21-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m211.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: uv\n",
      "Successfully installed uv-0.9.21\n"
     ]
    }
   ],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10380de6-2536-458d-82dc-55dc689d6818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 ms, sys: 13.4 ms, total: 30.3 ms\n",
      "Wall time: 39.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "!uv pip install vllm --torch-backend=auto --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a8c032-67c8-49c6-bc81-fe967b06e04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: vllm\n",
      "Version: 0.13.0\n",
      "Summary: A high-throughput and memory-efficient inference and serving engine for LLMs\n",
      "Home-page: https://github.com/vllm-project/vllm\n",
      "Author: vLLM Team\n",
      "Author-email: \n",
      "License-Expression: Apache-2.0\n",
      "Location: /opt/conda/lib/python3.12/site-packages\n",
      "Requires: aiohttp, anthropic, blake3, cachetools, cbor2, cloudpickle, compressed-tensors, depyf, diskcache, einops, fastapi, filelock, flashinfer-python, gguf, ijson, lark, llguidance, lm-format-enforcer, mcp, mistral_common, model-hosting-container-standards, msgspec, ninja, numba, numpy, openai, openai-harmony, opencv-python-headless, outlines_core, partial-json-parser, pillow, prometheus-fastapi-instrumentator, prometheus_client, protobuf, psutil, py-cpuinfo, pybase64, pydantic, python-json-logger, pyyaml, pyzmq, ray, regex, requests, scipy, sentencepiece, setproctitle, setuptools, six, tiktoken, tokenizers, torch, torchaudio, torchvision, tqdm, transformers, typing_extensions, watchfiles, xgrammar\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961d0180-a794-4adf-ad83-fc9ba0b48f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f40aae1c-0e4a-41b8-8e25-4e150d393e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_DATA_PATH = \"subtask1/dev\"\n",
    "TRAIN_DATA_PATH = \"subtask1/train\"\n",
    "OUTPUT_DIR = \"predictions/subtask_1\"\n",
    "LANG_CODES = [\n",
    "    \"amh\",\n",
    "    \"arb\",\n",
    "    \"ben\",\n",
    "    \"deu\",\n",
    "    \"eng\",\n",
    "    \"fas\",\n",
    "    \"hau\",\n",
    "    \"hin\",\n",
    "    \"ita\",\n",
    "    \"khm\",\n",
    "    \"mya\",\n",
    "    \"nep\",\n",
    "    \"ori\",\n",
    "    \"pan\",\n",
    "    \"pol\",\n",
    "    \"rus\",\n",
    "    \"spa\",\n",
    "    \"swa\",\n",
    "    \"tel\",\n",
    "    \"tur\",\n",
    "    \"urd\",\n",
    "    \"zho\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be33229-61a0-4a77-ab1d-24a31a5772d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-03 16:24:43 [utils.py:253] non-default args: {'disable_log_stats': True, 'model': 'meta-llama/Llama-3.1-8B-Instruct'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13631d020b4843e19cd13e6096af32c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-03 16:24:50 [model.py:514] Resolved architecture: LlamaForCausalLM\n",
      "INFO 01-03 16:24:50 [model.py:1661] Using max model len 131072\n",
      "INFO 01-03 16:24:50 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a1c4e9d3684596bffb9e272cd25432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa10b61606f1484baf7bcbddd121a880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4f5bac5a964cca9a6e92436f904923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903fba6bbad447d9b053ca0407d7053c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:24:53 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:24:53 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.0.11.30:38467 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:24:53 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:24:54 [gpu_model_runner.py:3562] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m /opt/conda/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m We recommend installing via `pip install torch-c-dlpack-ext`\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:24:56 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c0a9ef17f54c0f972b77ddbc05e016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d95a904e79470db462d5b597272db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408bf40323e94b1aba6631fa717661a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7d3c32e6084e8d8191f452dfd0b87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:25:20 [weight_utils.py:487] Time spent downloading weights for meta-llama/Llama-3.1-8B-Instruct: 24.159114 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a973f4295aea45c3932de19c0f3c788a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efc98d32ac843449741f13ddfbc5365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:25:34 [default_loader.py:308] Loading weights took 13.73 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:25:35 [gpu_model_runner.py:3659] Model loading took 14.9889 GiB memory and 40.147386 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:25:40 [backends.py:643] Using cache directory: /home/jovyan/.cache/vllm/torch_compile_cache/74f7e42122/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:25:40 [backends.py:703] Dynamo bytecode transform time: 4.91 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:25:45 [backends.py:261] Cache the graph of compile range (1, 8192) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:25:49 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 7.16 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:25:49 [monitor.py:34] torch.compile takes 12.06 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:25:50 [gpu_worker.py:375] Available KV cache memory: 23.81 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:25:50 [kv_cache_utils.py:1291] GPU KV cache size: 195,024 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:25:50 [kv_cache_utils.py:1296] Maximum concurrency for 131,072 tokens per request: 1.49x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:02<00:00, 23.26it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:01<00:00, 27.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:25:54 [gpu_model_runner.py:4587] Graph capturing finished in 4 secs, took 0.57 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=455)\u001b[0;0m INFO 01-03 16:25:54 [core.py:259] init engine (profile, create kv cache, warmup model) took 19.54 seconds\n",
      "INFO 01-03 16:25:55 [llm.py:360] Supported tasks: ['generate']\n",
      "CPU times: user 792 ms, sys: 263 ms, total: 1.06 s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# llm = LLM(model=\"Qwen/Qwen3-8B\", dtype=\"auto\", gpu_memory_utilization=0.9)\n",
    "llm = LLM(model=\"meta-llama/Llama-3.1-8B-Instruct\", dtype=\"auto\", gpu_memory_utilization=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4de58b3-fcf8-4023-8d67-b20f5af04de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873624fb855640e3ac967415b286e0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cb5c9d9f01475c8a8ff54cbe64d253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you have a toy box filled with different colored balls. Each ball represents a piece of information, like a number or a word. Now, imagine that these balls are all jumbled up inside the box and you can't see what's inside. That's kind of like how computers work, they have a lot of information stored inside them, but it's all jumbled up and hard to understand.\n",
      "Now, imagine that you have a special kind of ball that can do something really cool.\n",
      "CPU times: user 12.4 ms, sys: 5.85 ms, total: 18.2 ms\n",
      "Wall time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Can you explain quantum computing to a 5 year old?\"\n",
    "\n",
    "# Set sampling parameters for a simple, controlled response\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,  # Adjust for creativity; 0.0 for deterministic\n",
    "    max_tokens=100,  # Limit output length\n",
    "    top_p=0.95,\n",
    ")\n",
    "# Generate the response\n",
    "outputs = llm.generate(\n",
    "    [prompt], sampling_params\n",
    ")  # Pass as list for batching, even for one\n",
    "# Print the response\n",
    "response = outputs[0].outputs[0].text.strip()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "894b926c-4bc6-478b-8b68-7362c934c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_few_shot_examples(lang_code, n_per_class=15, seed=42):\n",
    "    train_file = os.path.join(TRAIN_DATA_PATH, f\"{lang_code}.csv\")\n",
    "\n",
    "    if not os.path.exists(train_file):\n",
    "        raise FileNotFoundError(f\"No training file for {lang_code}\")\n",
    "\n",
    "    df = pd.read_csv(train_file)\n",
    "\n",
    "    pos = df[df[\"polarization\"] == 1].sample(\n",
    "        n=n_per_class, random_state=seed\n",
    "    )\n",
    "    neg = df[df[\"polarization\"] == 0].sample(\n",
    "        n=n_per_class, random_state=seed\n",
    "    )\n",
    "\n",
    "    return pd.concat([pos, neg]).sample(frac=1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7bf06f2-1796-44f0-b75a-0820f1caa021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_few_shot_prefix(lang_code, few_shot_df):\n",
    "    header = f\"\"\"You are an expert in detecting polarized content in social media posts.\n",
    "\n",
    "Polarized content includes hate speech, toxicity, misogyny, gender-based violence,\n",
    "sarcasm, offensive speech, or strong us-vs-them divisions.\n",
    "\n",
    "The ISO 639 language code is \"{lang_code}\".\n",
    "\n",
    "Below are labeled examples.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    examples = []\n",
    "    for _, row in few_shot_df.iterrows():\n",
    "        label = \"Yes\" if row[\"polarization\"] == 1 else \"No\"\n",
    "        examples.append(\n",
    "            f\"Text: {row['text']}\\nAnswer: {label}\\n\"\n",
    "        )\n",
    "\n",
    "    examples_block = \"\\n\".join(examples)\n",
    "\n",
    "    footer = \"\"\"\n",
    "Now classify the following text.\n",
    "\n",
    "Text: {text}\n",
    "Answer (Yes or No only):\n",
    "\"\"\"\n",
    "\n",
    "    return header + examples_block + footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e5229b-829a-445e-9fd6-b8792692ed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Few-shot prefix ready\n"
     ]
    }
   ],
   "source": [
    "# Cache Prefixes Per Language\n",
    "FEW_SHOT_PREFIXES = {}\n",
    "\n",
    "for lang in LANG_CODES:\n",
    "    try:\n",
    "        few_shot_df = load_few_shot_examples(lang)\n",
    "        FEW_SHOT_PREFIXES[lang] = build_few_shot_prefix(lang, few_shot_df)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Skipping {lang}: {e}\")\n",
    "\n",
    "print(f\"‚úì Few-shot prefix ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2abe78a-5d59-4704-b522-2882cff98ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an expert in detecting polarized content in social media posts.\\n\\nPolarized content includes hate speech, toxicity, misogyny, gender-based violence,\\nsarcasm, offensive speech, or strong us-vs-them divisions.\\n\\nThe ISO 639 language code is \"hin\".\\n\\nBelow are labeled examples.\\n\\nText: ‡§π‡§ø‡§Ç‡§¶‡•Å‡§§‡•ç‡§µ ‡§ï‡•ã‡§à ‡§ß‡§∞‡•ç‡§Æ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à ‡§µ‡•ã ‡§§‡•ã ‡§Æ‡§æ‡§®‡§µ‡§§‡§æ ‡§î‡§∞ ‡§á‡§Ç‡§∏‡§æ‡§®‡§ø‡§Ø‡§§ ‡§ï‡•ã ‡§¨‡§ö‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡§ï‡•É‡§§‡§ø ‡§ï‡•á ‡§∏‡§æ‡§• ‡§¨‡§®‡§æ ‡§¨‡§Ç‡§ß‡§® ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à‡•§\\nAnswer: No\\n\\nText: #ZeeBiharJharkhand Darbhanga ‡§Æ‡•á‡§Ç Hindu ‡§≤‡§°‡§º‡§ï‡•Ä ‡§ï‡§æ Muslim ‡§≤‡§°‡§º‡§ï‡•á ‡§®‡•á ‡§ï‡§ø‡§Ø‡§æ ‡§Ö‡§™‡§π‡§∞‡§£, ‡§≤‡§µ-‡§ú‡§ø‡§π‡§æ‡§¶ ‡§ï‡§æ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§¶‡•á‡§ñ‡•á‡§Ç! Darbhanga ‡§Æ‡•á‡§Ç Hindu ‡§≤‡§°‡§º‡§ï‡•Ä ‡§ï‡§æ Muslim ‡§≤‡§°‡§º‡§ï‡•á ‡§®‡•á ‡§ï‡§ø‡§Ø‡§æ ‡§Ö‡§™‡§π‡§∞‡§£, ‡§≤‡§µ-‡§ú‡§ø‡§π‡§æ‡§¶ ‡§ï‡§æ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§¶‡•á‡§ñ‡•á‡§Ç! Darbhanga Muslim Boy Kidnapped Hindu Girl: Bihar ‡§ï‡•á Darbhanga ‡§ú‡§ø‡§≤‡•á ‡§ï‡•á ‡§Ö‡§≤‡•Ä‡§®‡§ó‡§∞ ‡§•‡§æ‡§®‡§æ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§∏‡•á ‡§è‡§ï ‡§ö‡•å‡§Ç‡§ï‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§î‡§∞‚Ä¶\\nAnswer: No\\n\\nText: ‡§Æ‡•Å‡§Ç‡§¨‡§à ‡§á‡§Ç‡§°‡§ø‡§Ø‡§Ç‡§∏ ‡§ï‡•ã ‡§≤‡§ó‡§æ ‡§¨‡§°‡§º‡§æ ‡§ù‡§ü‡§ï‡§æ, ‡§¨‡§æ‡§π‡§∞ ‡§π‡•Å‡§Ü ‡§Ø‡•á ‡§Æ‡•à‡§ö ‡§µ‡§ø‡§®‡§∞ ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä; ‡§∞‡§ø‡§™‡•ç‡§≤‡•á‡§∏‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§ê‡§≤‡§æ‡§® Image Source : PTI ‡§µ‡§ø‡§ó‡•ç‡§®‡•á‡§∂ ‡§™‡•Å‡§•‡•Å‡§∞ ‡§Æ‡•Å‡§Ç‡§¨‡§à ‡§á‡§Ç‡§°‡§ø‡§Ø‡§Ç‡§∏ ‡§ï‡•Ä ‡§ü‡•Ä‡§Æ ‡§ï‡•ã ‡§Ü‡§à‡§™‡•Ä‡§è‡§≤ 2025 ‡§ï‡•á ‡§∏‡•Ä‡§ú‡§® ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§¨‡§°‡§º‡§æ ‡§ù‡§ü‡§ï‡§æ 24 ‡§∏‡§æ‡§≤ ‡§ï‡•á ‡§¨‡§æ‡§è‡§Ç ‡§π‡§æ‡§• ‡§ï‡•á ‡§∞‡§ø‡§∏‡•ç‡§ü ‡§∏‡•ç‡§™‡§ø‡§®‡§∞ ‡§µ‡§ø‡§ó‡•ç‡§®‡•á‡§∂ ‡§™‡•Å‡§•‡•Å‡§∞ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§≤‡§ó‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§ö‡•ã‡§ü‡§ø‡§≤ ‡§π‡•ã‡§®‡•á ‡§ï‡•Ä ‡§µ‡§ú‡§π ‡§∏‡•á ‡§á‡§∏ ‡§∏‡•Ä‡§ú‡§® ‡§ï‡•á ‡§¨‡§æ‡§ï‡•Ä ‡§¨‡§ö‡•á ‡§Æ‡•à‡§ö‡•ã‡§Ç‚Ä¶\\nAnswer: No\\n\\nText: ‡§á‡§∏ ‡§¨‡§æ‡§∞ Gujarat ‡§ï‡•Ä ‡§ú‡§®‡§§‡§æ ‡§ï‡§ø‡§∏ ‡§≤‡§ø‡§è Vote ‡§ï‡§∞‡•á‡§ó‡•Ä? ‡§≠‡§æ‡§ú‡§™‡§æ ‡§Æ‡•ã‡§¶‡•Ä ‡§π‡§ø‡§Ç‡§¶‡•Å‡§§‡•ç‡§µ All the above üôèüôèüôèüôèüôèüôèüôè\\nAnswer: No\\n\\nText: #ZeeNews Agra Conversion: ‡§Ü‡§ó‡§∞‡§æ ‡§ß‡§∞‡•ç‡§Æ‡§æ‡§Ç‡§§‡§∞‡§£ ‡§Æ‡§æ‡§Æ‡§≤‡§æ.. ‡§á‡§∏‡•ç‡§≤‡§æ‡§Æ‡§ø‡§ï ‡§è‡§ú‡•á‡§Ç‡§°‡§æ DECODE! | Breaking News | Uttar Pradesh News Agra Conversion: ‡§Ü‡§ó‡§∞‡§æ ‡§ß‡§∞‡•ç‡§Æ‡§æ‡§Ç‡§§‡§∞‡§£ ‡§Æ‡§æ‡§Æ‡§≤‡§æ.. ‡§á‡§∏‡•ç‡§≤‡§æ‡§Æ‡§ø‡§ï ‡§è‡§ú‡•á‡§Ç‡§°‡§æ DECODE! | Breaking News | Uttar Pradesh News #agraconversion #upconversion #religionconversion Agra Conversion: ‡§Ü‡§ó‡§∞‡§æ ‡§ß‡§∞‡•ç‡§Æ‡§æ‡§Ç‡§§‡§∞‡§£ ‡§Æ‡§æ‡§Æ‡§≤‡•á‚Ä¶\\nAnswer: Yes\\n\\nText: ‡§´‡§°‡§£‡§µ‡•Ä‡§∏ ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§á‡§∏ ‡§´‡•à‡§∏‡§≤‡•á ‡§ï‡•á ‡§ñ‡§ø‡§≤‡§æ‡§´ ‡§ñ‡•Å‡§≤‡§ï‡§∞ ‡§ñ‡§°‡§º‡•á ‡§π‡•ã ‡§ó‡§è ‡§∞‡§æ‡§Æ‡§¶‡§æ‡§∏ ‡§Ü‡§†‡§µ‡§≤‡•á, ‡§¨‡•ã‡§≤‡•á- ‡§Ø‡•á ‡§ó‡§≤‡§§ ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞‡•Ä‡§Ø ‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§∞‡§æ‡§Æ‡§¶‡§æ‡§∏ ‡§Ü‡§†‡§µ‡§≤‡•á ‡§®‡•á ‚Äò‚Äò‡§≤‡§µ ‡§ú‡§ø‡§π‡§æ‡§¶‚Äô‚Äô ‡§ï‡•ã ‡§∞‡•ã‡§ï‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§ï‡§æ‡§®‡•Ç‡§® ‡§ï‡§æ ‡§Æ‡§∏‡•å‡§¶‡§æ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§ï‡§¶‡§Æ ‡§ï‡§æ ‡§∞‡§µ‡§ø‡§µ‡§æ‡§∞ ‡§ï‡•ã ‡§µ‡§ø‡§∞‡•ã‡§ß ‡§ï‡§ø‡§Ø‡§æ. ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§®‡•á ‡§∏‡§∞‡§ï‡§æ‡§∞‡•Ä ‡§∏‡§Ç‡§ï‡§≤‡•ç‡§™ ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç ‡§ï‡§π‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à ‡§ï‡§ø ‡§™‡•Å‡§≤‡§ø‡§∏‚Ä¶\\nAnswer: Yes\\n\\nText: Sirf 15 ‡§ï‡§∞‡•ã‡§°‡§º ‡§Æ‡•Å‡§∏‡§≤‡§Æ‡§æ‡§®‡•ã‡§Ç ‡§ï‡•ã ‡§π‡•Ä ‡§ó‡§ø‡§®‡•§\\nAnswer: No\\n\\nText: ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§ú‡§æ‡§¨ ‡§∂‡§¨‡•ç‡§¶ ‡§¨‡•ã‡§≤‡§æ ‡§π‡•à ‡§≤‡•á‡§ï‡§ø‡§® ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§¨‡•Å‡§∞‡•ç‡§ï‡§æ ‡§™‡§π‡§® ‡§∞‡§ñ‡§æ ‡§•‡§æ‡•§\\nAnswer: No\\n\\nText: ‡§á‡§∏ ‡§∏‡§¨ ‡§Æ‡§ø‡§°‡§ø‡§Ø‡§æ ‡§¨‡§Ç‡§¶ ‡§π‡•ã‡•§ ‡§¶‡•á‡§∂‡§¶‡•ç‡§∞‡•ã‡§π‡•Ä ‡§Æ‡§ø‡§°‡§ø‡§Ø‡§æ ‡§π‡•à‡•§\\nAnswer: Yes\\n\\nText: #‡§™‡§π‡§ö‡§æ‡§®_‡§™‡•ç‡§∞‡§≠‡•Å_‡§ï‡•Ä ‡§ï‡§¨‡•Ä‡§∞ ‡§™‡§∞‡§Æ‡•á‡§∂‡•ç‡§µ‡§∞ ‡§Æ‡§ó‡§π‡§∞ ‡§∏‡•á ‡§∏‡§∂‡§∞‡•Ä‡§∞ ‡§∏‡§§‡§≤‡•ã‡§ï ‡§ó‡§è‡•§ ‡§â‡§®‡§ï‡•á ‡§∂‡§∞‡•Ä‡§∞ ‡§ï‡•á ‡§∏‡•ç‡§•‡§æ‡§® ‡§™‡§∞ ‡§∏‡•Å‡§ó‡§Ç‡§ß‡§ø‡§§ ‡§´‡•Ç‡§≤ ‡§™‡§æ‡§è ‡§ó‡§è ‡§ú‡•ã ‡§ï‡§¨‡•Ä‡§∞ ‡§™‡§∞‡§Æ‡•á‡§∂‡•ç‡§µ‡§∞ ‡§ï‡•Ä ‡§Ü‡§ú‡•ç‡§û‡§æ ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ß‡§∞‡•ç‡§Æ‡•ã‡§Ç ‡§®‡•á ‡§Ü‡§™‡§∏ ‡§Æ‡•á‡§Ç ‡§≤‡•á‡§ï‡§∞ ‡§Æ‡§ó‡§π‡§∞ ‡§Æ‡•á‡§Ç 100 ‡§´‡•Å‡§ü ‡§ï‡•á ‡§Ö‡§Ç‡§§‡§∞ ‡§∏‡•á ‡§è‡§ï-‡§è‡§ï ‡§Ø‡§æ‡§¶‡§ó‡§æ‡§∞ ‡§¨‡§®‡§æ‡§à ‡§ú‡•ã ‡§Ü‡§ú ‡§≠‡•Ä ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§Æ‡§æ‡§® ‡§π‡•à‡•§ ‡§Ø‡§π ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ß‡§∞‡•ç‡§Æ‡•ã‡§Ç ‡§π‡§ø‡§Ç‡§¶‡•Å‡§ì‡§Ç ‡§î‡§∞ ‡§Æ‡•Å‡§∏‡§≤‡§Æ‡§æ‡§®‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§Ü‡§™‡§∏‡•Ä ‡§≠‡§æ‡§à‡§ö‡§æ‡§∞‡•á ‡§µ ‡§∏‡§¶‡•ç‡§≠‡§æ‡§µ‡§®‡§æ ‡§ï‡•Ä ‡§è‡§ï ‡§Æ‡§ø‡§∏‡§æ‡§≤ ‡§ï‡§æ ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£ ‡§π‡•à‡•§ ‡§ó‡§∞‡•Ä‡§¨, ‡§¨‡§ø‡§∞‡§∏‡§ø‡§Ç‡§ò ‡§¨‡§ò‡•á‡§≤‡§æ ‡§ï‡§∞‡•à ‡§¨‡•Ä‡§®‡§§‡•Ä, ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§ñ‡§æ‡§Å‡§® ‡§™‡§†‡§æ‡§®‡§æ ‡§π‡•ã‡•§ ‡§¶‡•ã ‡§ö‡§¶‡§∞‡§ø ‡§¨‡§ï‡§∏‡•Ä‡§∏ ‡§ï‡§∞‡•Ä ‡§π‡•à, ‡§¶‡•Ä‡§®‡§æ ‡§Ø‡•å‡§π ‡§™‡•ç‡§∞‡§µ‡§æ‡§Ç‡§®‡§æ ‡§π‡•ã\\nAnswer: Yes\\n\\nText: ‡§¶‡•á‡§∂‡§¶‡•ç‡§∞‡•ã‡§π‡•Ä ‡§ï‡•á ‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§® ‡§Æ‡•á‡§Ç, ‡§∞‡§æ‡§ú‡•ç‡§Ø‡§™‡§æ‡§≤ ‡§Æ‡•à‡§¶‡§æ‡§® ‡§Æ‡•á‡§Ç!! Pass it on\\nAnswer: Yes\\n\\nText: ‡§π‡§ø‡§Æ‡§æ‡§Ç‡§ö‡§≤ ‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ ‡§Æ‡•á‡§Ç ‡§ñ‡§æ‡§≤‡§ø‡§∏‡•ç‡§§‡§æ‡§®‡•Ä ‡§ù‡§Ç‡§°‡§æ ‡§≤‡§ó‡§æ‡§®‡•á ‡§ï‡•á ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§≤‡§ó‡§æ UAPA\\nAnswer: No\\n\\nText: ‡§â‡§®‡§ï‡•ã ‡§ú‡§æ‡§®‡§µ‡§∞ ‡§∏‡•á ‡§®‡§π‡•Ä ‡§Æ‡•Å‡§∏‡§≤‡§Æ‡§æ‡§®‡•ã‡§Ç ‡§∏‡•á ‡§π‡•à\\nAnswer: Yes\\n\\nText: ‡§¶‡•á‡§∂‡§¶‡•ç‡§∞‡•ã‡§π‡•Ä ‡§ï‡•ã ‡§¶‡•á‡§∂ ‡§ï‡•Ä ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§ú‡§º‡§∞‡•Ç‡§∞‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§\\nAnswer: Yes\\n\\nText: #ZeeNews Agra Conversion Case: ‡§Ü‡§ó‡§∞‡§æ ‡§ß‡§∞‡•ç‡§Æ‡§æ‡§Ç‡§§‡§∞‡§£ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§π‡•à‡§∞‡§æ‡§® ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§ñ‡•Å‡§≤‡§æ‡§∏‡§æ! | Abdul Rehman Agra Conversion Case: ‡§Ü‡§ó‡§∞‡§æ ‡§ß‡§∞‡•ç‡§Æ‡§æ‡§Ç‡§§‡§∞‡§£ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§π‡•à‡§∞‡§æ‡§® ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§ñ‡•Å‡§≤‡§æ‡§∏‡§æ! | Abdul Rehman Agra Conversion Case: ‡§Ü‡§ó‡§∞‡§æ ‡§ß‡§∞‡•ç‡§Æ‡§æ‡§Ç‡§§‡§∞‡§£ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§Æ‡•á‡§Ç ‡§π‡•à‡§∞‡§æ‡§® ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§ñ‡•Å‡§≤‡§æ‡§∏‡§æ! | Abdul Rehman ‡§Ü‡§ó‡§∞‡§æ ‡§ß‡§∞‡•ç‡§Æ‡§æ‡§Ç‡§§‡§∞‡§£ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§ï‡§æ‚Ä¶\\nAnswer: Yes\\n\\nText: ‡§Ö‡§¨ ‡§∏‡§≠‡•Ä ‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ‡§ì‡§Ç ‡§î‡§∞ ‡§ï‡§Ç‡§™‡§®‡§ø‡§Ø‡•ã‡§Ç ‡§™‡§∞ ‡§Ø‡§π‡•Ä ‡§®‡§ø‡§Ø‡§Æ ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è ‡§î‡§∞ 20% ‡§Æ‡•Å‡§∏‡§≤‡§Æ‡§æ‡§®‡•ã‡§Ç ‡§ï‡•ã ‡§®‡•å‡§ï‡§∞‡•Ä ‡§™‡§∞ ‡§∞‡§ñ‡§®‡§æ ‡§π‡•ã‡§ó‡§æ‡•§\\nAnswer: No\\n\\nText: ‡§¶‡•Ä‡§¶‡•Ä‡§π‡§∞‡•Å ‡§õ‡§ø‡§Æ‡•á‡§ï ‡§§‡•Ä‡§∞ \\'‡§¨‡§ø‡§¶‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§≠‡§®‡•ç‡§¶‡§æ ‡§¨‡•Å‡§∞‡•ç‡§ï‡§æ ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£\\' ‡§≠‡§®‡•á ‡§™‡§®‡§ø ‡§π‡•ã ‡§π‡•ã ‡§≠‡§®‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ‡•§‡•§ #‡§â‡§¶‡•á‡§ï\\nAnswer: Yes\\n\\nText: ‡§ú‡•ã ‡§¶‡•á‡§∂‡§¶‡•ç‡§∞‡•ã‡§π‡•Ä ‡§π‡•à ‡§µ‡§π‡•Ä ‡§§‡§ø‡§∞‡§Ç‡§ó‡§æ ‡§µ‡§ø‡§∞‡•ã‡§ß‡•Ä ‡§π‡•à..!!  ‡§ú‡•ã ‡§≠‡•ç‡§∞‡§∑‡•ç‡§ü ‡§π‡•à ‡§â‡§®‡•ç‡§π‡•á‡§Ç ‡§π‡•Ä ‡§Æ‡•ã‡§¶‡•Ä ‡§î‡§∞ ED ‡§∏‡•á ‡§ï‡§∑‡•ç‡§ü ‡§π‡•à..!!!\\nAnswer: Yes\\n\\nText: ‡§Ö‡§¨ ‡§µ‡§ø‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§≠‡•Ä #‡§π‡§ø‡§Ç‡§¶‡•Å‡§§‡•ç‡§µ ‡§ï‡§æ ‡§ú‡§≤‡§µ‡§æ ‡§¶‡§ø‡§ñ‡•á‡§ó‡§æ...‚úäüí™üí™\\nAnswer: No\\n\\nText: ‡§ú‡§ø‡§π‡§æ‡§¶ ‡§î‡§∞ ‡§Ü‡§§‡§Ç‡§ï‡•Ä ‡§Æ‡§æ‡§®‡§∏‡§ø‡§ï‡§§‡§æ ‡§õ‡•ã‡§°‡§º ‡§¶‡•á ‡§§‡•ã ‡§≠‡§æ‡§∞‡§§ ‡§ú‡•à‡§∏‡§æ ‡§ï‡•ã‡§à ‡§ñ‡•Å‡§∂ ‡§π‡§æ‡§≤ ‡§¶‡•á‡§∂ ‡§¶‡•Ç‡§∏‡§∞‡§æ nhi ‡§π‡•ã‡§ó‡§æ ‡•§\\nAnswer: Yes\\n\\nText: UPI ‡§Ø‡•Ç‡§ú‡§∞‡•ç‡§∏ ‡§ï‡•ã ‡§≤‡§ó‡§æ ‡§¨‡§°‡§º‡§æ ‡§ù‡§ü‡§ï‡§æ! ‡§Ö‡§¨ ‡§Ö‡§≤‡§ó-‡§Ö‡§≤‡§ó ‡§∏‡§∞‡•ç‡§µ‡§ø‡§∏‡•á‡§ú ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ö‡•Å‡§ï‡§æ‡§®‡•Ä ‡§π‡•ã‡§ó‡•Ä ‡§´‡•Ä‡§∏, ‡§á‡§∏ ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§®‡•á ‡§ï‡•Ä ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§Ü‡§ú ‡§ï‡•á ‡§∏‡§Æ‡§Ø ‡§Æ‡•á‡§Ç UPI ‡§π‡§Æ‡§æ‡§∞‡•Ä ‡§ú‡§ø‡§Ç‡§¶‡§ó‡•Ä ‡§ï‡§æ ‡§è‡§ï ‡§Ö‡§π‡§Æ ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§¨‡§® ‡§ó‡§Ø‡§æ ‡§π‡•à. ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø ‡§∞‡•ã‡§ú‡§æ‡§®‡§æ ‡§î‡§∏‡§§‡§® ‡§ï‡§∞‡•Ä‡§¨ 60 ‡§∏‡•á 80 ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ ‡§≤‡•á‡§®‡§¶‡•á‡§® ‡§Ø‡•Ç‡§™‡•Ä‡§Ü‡§à ‡§ï‡•á ‡§ú‡§∞‡§ø‡§è ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•à. ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á‡§Ç ‡§∞‡•ã‡§ú‡§æ‡§®‡§æ ‡§ï‡§∞‡•ã‡§°‡§º‡•ã‡§Ç ‡§Ø‡•Ç‡§™‡•Ä‡§Ü‡§à ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§ú‡•à‡§ï‡•ç‡§∂‡§® ‡§π‡•ã ‡§∞‡§π‡•á ‡§π‡•à‡§Ç, ‡§ú‡§ø‡§®‡§ï‡•á ‡§ú‡§∞‡§ø‡§è‚Ä¶\\nAnswer: No\\n\\nText: **Bharatiya - Shuddha Hindi** **Unadulterated / Shuddha Hindi EP 300** **‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø - ‡§∂‡•Å‡§¶‡•ç‡§ß ‡§π‡§ø‡§Ç‡§¶‡•Ä** Episode 300 ‡§™‡•ç‡§∞‡§ï‡§∞‡§£ ‡•©‡•¶‡•¶ Word / Phrase: hinduism hindu ( ‡§π‡§ø‡§®‡•ç‡§¶‡•Ç / ‡§π‡§ø‡§Ç‡§¶‡•Ç ) Acceptable (partially adulterated) ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞‡•ç‡§Ø ( ‡§Ö‡§Ç‡§∂‡§§: ‡§Ö‡§∂‡•Å‡§¶‡•ç‡§ß ‡§Ö‡§•‡§µ‡§æ ‡§Æ‡§ø‡§≤‡§æ‡§µ‡§ü‡•Ä )([Middle Persian (from Old Persian (from Sanskrit))](@URL dharma ( ‡§ß‡§∞‡•ç‡§Æ ) ‚úÖ ([Sanskrit](@URL sanatana dharma ( ‡§∏‡§®‡§æ‡§§‡§® ‡§ß‡§∞‡•ç‡§Æ ) ‚úÖ ([Sanskrit](@URL # ‡§ß‡§∞‡•ç‡§Æ‡•ã ‡§∞‡§ï‡•ç‡§∑‡§§‡§ø ‡§∞‡§ï‡•ç‡§∑‡§ø‡§§‡§É‡•§ ‡§Ø‡§¶‡§æ ‡§Ø‡§¶‡§æ ‡§π‡§ø ‡§ß‡§∞‡•ç‡§Æ‡§∏‡•ç‡§Ø ‡§ó‡•ç‡§≤‡§æ‡§®‡§ø‡§∞‡•ç‡§≠‡§µ‡§§‡§ø ‡§≠‡§æ‡§∞‡§§‡•§ ‡§Ö‡§≠‡•ç‡§Ø‡•Å‡§§‡•ç‡§•‡§æ‡§®‡§Æ‡§ß‡§∞‡•ç‡§Æ‡§∏‡•ç‡§Ø ‡§§‡§¶‡§æ‡§§‡•ç‡§Æ‡§æ‡§®‡§Ç ‡§∏‡•É‡§ú‡§æ‡§Æ‡•ç‡§Ø‡§π‡§Æ‡•ç‡•• ‡§™‡§∞‡§ø‡§§‡•ç‡§∞‡§æ‡§£‡§æ‡§Ø ‡§∏‡§æ‡§ß‡•Ç‡§®‡§æ‡§Ç ‡§µ‡§ø‡§®‡§æ‡§∂‡§æ‡§Ø ‡§ö ‡§¶‡•Å‡§∑‡•ç‡§ï‡•É‡§§‡§æ‡§Æ‡•ç‡•§ ‡§ß‡§∞‡•ç‡§Æ‡§∏‡§Ç‡§∏‡•ç‡§•‡§æ‡§™‡§®‡§æ‡§∞‡•ç‡§•‡§æ‡§Ø ‡§∏‡§Ç‡§≠‡§µ‡§æ‡§Æ‡§ø ‡§Ø‡•Å‡§ó‡•á ‡§Ø‡•Å‡§ó‡•á‡••\\nAnswer: No\\n\\nText: #ZeeNews Breaking News: ‡§Ø‡•Ç‡§™‡•Ä ‡§ï‡•Ä ‡§ú‡§®‡§§‡§æ ‡§ï‡•ã ‡§¨‡•ú‡§æ ‡§ù‡§ü‡§ï‡§æ! | Uttar Pradesh News | CM Yogi | Hindi News | Latest News Breaking News: ‡§Ø‡•Ç‡§™‡•Ä ‡§ï‡•Ä ‡§ú‡§®‡§§‡§æ ‡§ï‡•ã ‡§¨‡•ú‡§æ ‡§ù‡§ü‡§ï‡§æ! | Uttar Pradesh News | CM Yogi | Hindi News | Latest News #breakingnews #upelectricity #electricityrates UP Electricity Price Hike: ‡§Ø‡•Ç‡§™‡•Ä ‡§ï‡•Ä ‡§ú‡§®‡§§‡§æ ‡§ï‡•ã‚Ä¶\\nAnswer: No\\n\\nText: ‡§ï‡§ü‡•ç‡§ü‡§∞ ‡§à‡§Æ‡§æ‡§®‡§¶‡§æ‡§∞ ‡§Ö‡§∞‡•ç‡§•‡§æ‡§§ ‡§à‡§Æ‡§æ‡§®‡§¶‡§æ‡§∞‡•Ä ‡§ï‡•á ‡§ñ‡§ø‡§≤‡§æ‡§´ ‡§ú‡§ø‡§π‡§æ‡§¶ ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡§æ‡•§ üòÇü§£\\nAnswer: No\\n\\nText: ‡§Æ‡•á‡§∞‡•á ‡§Ö‡§≤‡•ç‡§≤‡§æ‡§π ‡§§‡•Å‡§®‡•á ‡§Æ‡•Å‡§∏‡§≤‡§Æ‡§æ‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§Ü‡§Ç‡§ñ‡•á‡§Ç ‡§§‡•ã ‡§ñ‡•ã‡§≤ ‡§¶‡•Ä ‡§π‡•à, ‡§Ö‡§¨ ‡§Æ‡•Å‡§§‡•ç‡§§‡§æ‡§π‡§ø‡§¶ ‡§≠‡•Ä ‡§ï‡§∞ ‡§¶‡•á‡§Ç, ‡§Ü‡§Æ‡•Ä‡§®\\nAnswer: No\\n\\nText: ‡§ú‡•ã ‡§∞‡§æ‡§Æ ‡§ï‡•ã ‡§≤‡§æ‡§è ‡§π‡•à‡§Ç.... #DeshNahinJhukneDenge ‡§Ö‡§Ø‡•ã‡§ß‡•ç‡§Ø‡§æ ‡§∏‡•á ‡§ö‡•Å‡§®‡§æ‡§µ, ‡§π‡§ø‡§Ç‡§¶‡•Å‡§§‡•ç‡§µ ‡§™‡§∞ ‡§¶‡§æ‡§Ç‡§µ ? #DeshNahinJhukneDenge 7.57 PM\\nAnswer: Yes\\n\\nText: ‡§ú‡•ã ‡§Ö‡§∏‡§≤‡•Ä ‡§¶‡•á‡§∂‡§≠‡§ï‡•ç‡§§ ‡§π‡•à‡§Ç, ‡§µ‡•á ‡§Ö‡§™‡§®‡•Ä ‡§§‡•à‡§Ø‡§æ‡§∞‡•Ä ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç, ‡§ú‡•ã ‡§ó‡§¶‡•ç‡§¶‡§æ‡§∞ ‡§¶‡•á‡§∂‡§¶‡•ç‡§∞‡•ã‡§π‡•Ä ‡§π‡•à‡§Ç, ‡§µ‡•á ‡§∏‡§°‡§º‡§ï‡•ã‡§Ç ‡§™‡§∞ ‡§§‡•ã‡§°‡§º‡§´‡•ã‡§°‡§º ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§ #iSupportAgnipath\\nAnswer: Yes\\n\\nText: ‡§∏‡•Å‡§™‡•ç‡§∞‡•Ä‡§Æ ‡§ï‡•ã‡§∞‡•ç‡§ü ‡§ï‡•á ‡§´‡•à‡§∏‡§≤‡•á ‡§∏‡•á JDU ‡§ï‡•ã ‡§≤‡§ó ‡§ó‡§Ø‡§æ ‡§ù‡§ü‡§ï‡§æ! ‡§¨‡§°‡§º‡•á ‡§´‡•à‡§∏‡§≤‡•á ‡§∏‡•á ‡§ú‡§æ‡§®‡•á‡§Ç ‡§ï‡§ø‡§∏‡§ï‡§æ ‡§ü‡•Ç‡§ü‡§æ ‡§∏‡§™‡§®‡§æ Bihar MLC By Election: ‡§¨‡§ø‡§π‡§æ‡§∞ ‡§µ‡§ø‡§ß‡§æ‡§® ‡§™‡§∞‡§ø‡§∑‡§¶ ‡§ï‡•Ä ‡§ñ‡§æ‡§≤‡•Ä ‡§π‡•Å‡§à ‡§è‡§ï ‡§∏‡•Ä‡§ü ‡§ï‡•á ‡§≤‡§ø‡§è NDA ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ ‡§ú‡•á‡§°‡•Ä‡§Ø‡•Ç ‡§â‡§Æ‡•ç‡§Æ‡•Ä‡§¶‡§µ‡§æ‡§∞ ‡§≤‡§≤‡§® ‡§™‡•ç‡§∞‡§∏‡§æ‡§¶ ‡§ï‡•á ‡§®‡§ø‡§∞‡•ç‡§µ‡§ø‡§∞‡•ã‡§ß ‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§ö‡§® ‡§ï‡•Ä ‡§ò‡•ã‡§∑‡§£‡§æ ‡§ü‡§≤ ‡§ó‡§à ‡§π‡•à. ‡§á‡§∏ ‡§â‡§™ ‡§ö‡•Å‡§®‡§æ‡§µ ‡§ï‡§æ ‡§®‡§§‡•Ä‡§ú‡§æ ‡§ò‡•ã‡§∑‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ ‡§∏‡•Å‡§™‡•ç‡§∞‡•Ä‡§Æ ‡§ï‡•ã‡§∞‡•ç‡§ü ‡§®‡•á ‡§´‡§ø‡§≤‡§π‡§æ‡§≤ ‡§Ö‡§ó‡§≤‡•Ä‚Ä¶\\nAnswer: Yes\\n\\nText: ‡§Æ‡§æ‡§ú‡§ø‡§¶ ‡§π‡•à‡§¶‡§∞‡•Ä ‡§ï‡§≠‡•Ä ‡§≠‡•Ä ‡§ö‡•Ä‡§® ‡§Æ‡•á‡§Ç ‡§Æ‡•Å‡§∏‡§≤‡§Æ‡§æ‡§®‡•ã‡§Ç ‡§™‡§∞ ‡§π‡•ã‡§∞‡§π‡•á ‡§ú‡•Å‡§≤‡•ç‡§Æ ‡§™‡§∞ ‡§è‡§ï ‡§∂‡§¨‡•ç‡§¶ ‡§≠‡•Ä ‡§¨‡•ã‡§≤‡§®‡•á ‡§π‡§ø‡§Æ‡•ç‡§Æ‡§§ ‡§®‡§π‡•Ä ‡§ï‡§∞‡§§?‡•§\\nAnswer: No\\n\\nText: ‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞‡•Ä ‡§Æ‡•Å‡§∏‡§≤‡§Æ‡§æ‡§®‡•ã‡§Ç ‡§ï‡•ã ‡§≤‡•á‡§ï‡§∞ ‡§Ø‡•á ‡§ï‡•ç‡§Ø‡§æ ‡§¨‡•ã‡§≤ ‡§ó‡§è AIMIM ‡§ö‡•Ä‡§´ ‡§Ö‡§∏‡§¶‡•Å‡§¶‡•ç‡§¶‡•Ä‡§® ‡§ì‡§µ‡•à‡§∏‡•Ä? ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§∏‡•á ‡§ï‡§∞ ‡§¶‡•Ä ‡§¨‡§°‡§º‡•Ä ‡§Æ‡§æ‡§Ç‡§ó Pahalgam Terror Attack: ‡§ú‡§Æ‡•ç‡§Æ‡•Ç-‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§ï‡•á ‡§™‡§π‡§≤‡§ó‡§æ‡§Æ ‡§Æ‡•á‡§Ç 22 ‡§Ö‡§™‡•ç‡§∞‡•à‡§≤ ‡§ï‡•ã ‡§π‡•Å‡§è ‡§Ü‡§§‡§Ç‡§ï‡§µ‡§æ‡§¶‡•Ä ‡§π‡§Æ‡§≤‡•á ‡§Æ‡•á‡§Ç 26 ‡§™‡§∞‡•ç‡§Ø‡§ü‡§ï‡•ã‡§Ç ‡§ï‡•Ä ‡§ó‡•ã‡§≤‡•Ä ‡§Æ‡§æ‡§∞‡§ï‡§∞ ‡§π‡§§‡•ç‡§Ø‡§æ ‡§ï‡§∞ ‡§¶‡•Ä ‡§ó‡§à. ‡§á‡§∏ ‡§Ö‡§Æ‡§æ‡§®‡§µ‡•Ä‡§Ø ‡§π‡§Æ‡§≤‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§Æ‡•á‡§Ç ‡§∞‡§π ‡§∞‡§π‡•á ‡§Æ‡•Å‡§∏‡§≤‡§Æ‡§æ‡§®‡•ã‡§Ç ‡§®‡•á ‡§á‡§∏ ‡§ò‡§ü‡§®‡§æ ‡§ï‡§æ ‡§ñ‡•Å‡§≤‡§ï‡§∞ ‡§µ‡§ø‡§∞‡•ã‡§ß‚Ä¶\\nAnswer: Yes\\n\\nNow classify the following text.\\n\\nText: {text}\\nAnswer (Yes or No only):\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEW_SHOT_PREFIXES.get('hin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "059efd69-376c-41f5-ae96-c55495c9d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_prompt(text, language_code):\n",
    "#     \"\"\"Create a zero-shot prompt for polarization detection.\"\"\"\n",
    "#     prompt = f\"\"\"You are an expert in detecting polarized content in social media posts. Polarized content is characterized as something that contains hate speech OR toxicity OR misogyny OR gender-based violence OR sarcastic speech OR it creates sharp division and hostility between social, political, or identity groups OR it expresses extreme opinions and viewpoints OR offensive speech\n",
    "# Task: Determine if the following text contains polarized opinion. The ISO 639 language code of the text is \"{language_code}\"\n",
    "\n",
    "# Text: {text}\n",
    "\n",
    "# Answer with ONLY \"Yes\" if the text contains polarized opinion, or \"No\" if it does not contain polarized opinion. Do not provide any explanation. Do not include any reasoning, explanation, or thinking process. Just output 'Yes' or 'No'\"\"\"\n",
    "#     return prompt\n",
    "\n",
    "def create_prompt(text, language_code):\n",
    "    prefix = FEW_SHOT_PREFIXES[language_code]\n",
    "    return prefix.format(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae8d891e-3070-4d40-a071-35d79fbcc16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an expert in detecting polarized content in social media posts.\\n\\nPolarized content includes hate speech, toxicity, misogyny, gender-based violence,\\nsarcasm, offensive speech, or strong us-vs-them divisions.\\n\\nThe ISO 639 language code is \"eng\".\\n\\nBelow are labeled examples.\\n\\nText: Putin This, Putin that, i just care about Putin this cash in my pockets\\nAnswer: No\\n\\nText: Jim, I hope you take Angela Micheles spot on MSNBC!\\nAnswer: No\\n\\nText: Exactly, but still bombing gaza, west bank and Lebanon. \"Cease fire\" my arse!\\nAnswer: No\\n\\nText: AOC and Kamala Harris dont really care.\\nAnswer: No\\n\\nText: Apparently the Hamas attack was the weathers fault.\\nAnswer: Yes\\n\\nText: If Israel didnt want Tel Aviv to be hit with missiles, it shouldnt be a genocidal apartheid state\\nAnswer: Yes\\n\\nText: Trumps trial to start March 4 on election interference\\nAnswer: No\\n\\nText: Support for lgbt Americsns includes those living in red states\\nAnswer: No\\n\\nText: They want to be able to point to this and say to their cult its an insurrection by Democrats this time.\\nAnswer: Yes\\n\\nText: \"Ceasefire\" lol not even close, its just palestines unconditional surrender theyre repackaging as a ceasefire to make it look better.\\nAnswer: Yes\\n\\nText: EU sanctions Russian military intelligence for hybrid attacks\\nAnswer: Yes\\n\\nText: Just follow our neighbour, Singapore. This is basic thing in border security and enforcement.\\nAnswer: No\\n\\nText: The first few weeks of 2025 have felt distinctly dystopian and thats even if you account for the ever present background uneasiness of climate change, the rise of far right populism, and anything Trump says. Lets commiserate and hope together\\nAnswer: Yes\\n\\nText: The musk vs maga stuff is fun to watch but it does stoke anti immigration sentiment in trumps base and maybe in new spaces.\\nAnswer: Yes\\n\\nText: Only Bungoma rigged election to defeat Azimio or else hungetoboa. And we cant ignore that\\nAnswer: Yes\\n\\nText: voted on multiple accounts call that voter fraud\\nAnswer: No\\n\\nText: So we going with the rigged election line for other countries now too ?\\nAnswer: Yes\\n\\nText: How would you feel about a mass deportation of my sperm?\\nAnswer: Yes\\n\\nText: Yes! Check your open borders for the outbreaks.\\nAnswer: No\\n\\nText: I have just joined as I had to get off X. It has been rigged to expose you to total racist and extreme right wing ideology.\\nAnswer: Yes\\n\\nText: Forget impeachment, how about making a difference to peoples lifes?\\nAnswer: No\\n\\nText: Thank you, Patty and Senate Democrats!!\\nAnswer: No\\n\\nText: Keep up the good fight zelensky ukraine\\nAnswer: No\\n\\nText: All the spoilers are on Fox News daily.\\nAnswer: No\\n\\nText: What good does that do if she loses the swing states again?\\nAnswer: No\\n\\nText: Maybe its time to stop giving red states our money?\\nAnswer: Yes\\n\\nText: Never call the fire fighters if your house burns down. Never use the roads outside your home. Never call the police for yourself. All socialism.\\nAnswer: Yes\\n\\nText: A valid protest of a rigged election, that ended up having some sort of kerfuffle.\\nAnswer: Yes\\n\\nText: How do I join Hamas?\\nAnswer: No\\n\\nText: the woke mob is making me publicly acknowledge the sovereign rights of Israel\\nAnswer: Yes\\n\\nNow classify the following text.\\n\\nText: hello, how are you?\\nAnswer (Yes or No only):\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt = create_prompt(\"hello, how are you?\", \"eng\")\n",
    "test_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76563429-ff0d-412e-b9bb-beff3d600ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response):\n",
    "    \"\"\"Parse model response to binary label\"\"\"\n",
    "    if response is None:\n",
    "        return 0  # Default to non-polarized if error\n",
    "\n",
    "    response_lower = response.lower().strip()\n",
    "    # Remove any potential <think> tags (for Qwen3)\n",
    "    response_lower = re.sub(\n",
    "        r\"<think>.*?</think>\", \"\", response_lower, flags=re.DOTALL\n",
    "    ).strip()\n",
    "\n",
    "    if \"yes\" in response_lower:\n",
    "        return 1\n",
    "    elif \"no\" in response_lower:\n",
    "        return 0\n",
    "    else:\n",
    "        print(f\"Unclear response: {response}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9684500d-869f-4ea4-b1b4-a06fee4c0d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_language(lang_code):\n",
    "    \"\"\"Process one language file with batched inference\"\"\"\n",
    "    input_file = os.path.join(DEV_DATA_PATH, f\"{lang_code}.csv\")\n",
    "\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"File not found: {input_file}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"\\nProcessing {lang_code}: {len(df)} samples\")\n",
    "\n",
    "    # Prepare all prompts as a list\n",
    "    prompts = [create_prompt(row[\"text\"], lang_code) for _, row in df.iterrows()]\n",
    "\n",
    "    # Define sampling params for fast, short outputs\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.0,  # Deterministic\n",
    "        # top_p=1.0,\n",
    "        # max_tokens=5,  # Plenty for \"Yes\"/\"No\", prevents over-generation\n",
    "        # skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # Batched generation - vLLM handles batching automatically\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    # Extract and parse responses\n",
    "    predictions = []\n",
    "    for idx, output in enumerate(outputs):\n",
    "        response = output.outputs[0].text.strip()  # Get the generated text\n",
    "        prediction = parse_response(response)\n",
    "        post_id = df.iloc[idx][\"id\"]\n",
    "        predictions.append({\"id\": post_id, \"polarization\": prediction})\n",
    "\n",
    "    return pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50af809e-ea80-4eff-9694-a5db082a003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_zip(lang_codes_to_process=None):\n",
    "    \"\"\"Create submission zip file\"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    if lang_codes_to_process is None:\n",
    "        lang_codes_to_process = LANG_CODES\n",
    "\n",
    "    for lang_code in lang_codes_to_process:\n",
    "        pred_df = process_language(lang_code)\n",
    "\n",
    "        if pred_df is not None:\n",
    "            output_file = os.path.join(OUTPUT_DIR, f\"pred_{lang_code}.csv\")\n",
    "            pred_df.to_csv(output_file, index=False)\n",
    "            print(f\"Saved predictions to {output_file}\")\n",
    "\n",
    "    zip_filename = \"subtask_1.zip\"\n",
    "    with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file in os.listdir(OUTPUT_DIR):\n",
    "            if file.startswith(\"pred_\") and file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(OUTPUT_DIR, file)\n",
    "                zipf.write(file_path, os.path.join(\"subtask_1\", file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa8d9f48-74b8-40be-ad67-d4fa33f68707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6b56976a4841bfb37715dedb1a5cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704c986ddb2047bda9765d314cdb9e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "CPU times: user 7.12 ms, sys: 5.59 ms, total: 12.7 ms\n",
      "Wall time: 392 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sample test\n",
    "test_text = \"I hate the way you talk, I hate the way you walk\"\n",
    "# test_text = \"‡§π‡§æ‡§Ø, ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç? ‡§â‡§Æ‡•ç‡§Æ‡•Ä‡§¶ ‡§π‡•à ‡§Ü‡§™ ‡§†‡•Ä‡§ï ‡§π‡•ã‡§Ç‡§ó‡•á?\"\n",
    "prompt = create_prompt(test_text, \"eng\")\n",
    "prompts = [prompt]\n",
    "test_params = SamplingParams(\n",
    "    temperature=0.0,  # Deterministic\n",
    "    # top_p=1.0,\n",
    "    # max_tokens=500,  # Plenty for \"Yes\"/\"No\", prevents over-generation\n",
    "    # skip_special_tokens=True\n",
    ")\n",
    "# Batched generation - vLLM handles batching automatically\n",
    "test_outputs = llm.generate(prompts, test_params)\n",
    "for out in test_outputs:\n",
    "    response = out.outputs[0].text.strip()\n",
    "    prediction = parse_response(response)\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39150dc4-b21d-4daa-bc5c-a891e9b8f683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing amh: 166 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffacf2ceecd942348437b6ff0fe20b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6012fd42163345f0a38d8160d5b33519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/166 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_amh.csv\n",
      "\n",
      "Processing arb: 169 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b8b5aad0cd442aa7f6f8924c5ab853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd85c49b27f4251b4d57fce979fbc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/169 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclear response: ```python\n",
      "def classify_text(text):\n",
      "    # List of polarized content examples\n",
      "Saved predictions to predictions/subtask_1/pred_arb.csv\n",
      "\n",
      "Processing ben: 166 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824f98d1195f4274a79d5dec6f965710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1981809b9bd446d88229e4947a53da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/166 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_ben.csv\n",
      "\n",
      "Processing deu: 159 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baead442d70f473489ad6ef1bb0fc40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2da34127f5d40358f20ba0ae746b34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/159 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_deu.csv\n",
      "\n",
      "Processing eng: 160 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc845345fd74b32910fdbae6ba82b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c46639ababe48d4a2a356a476b5aba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/160 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_eng.csv\n",
      "\n",
      "Processing fas: 164 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e009a2ffde42489a9e6fe51997dd6257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cc4b8659a04745ac800220cb2bf63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/164 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_fas.csv\n",
      "\n",
      "Processing hau: 182 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213ab233c1a7426e9fa8b1a9ea4319aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b167d4bcc34d2f8217fe83749a4b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/182 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_hau.csv\n",
      "\n",
      "Processing hin: 137 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb724a2217b3402fb58f818eb13f7c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83cb44339f054466b0d496202716d3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/137 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_hin.csv\n",
      "\n",
      "Processing ita: 166 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5566568cc9ce477abb65d0b2ed61fd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99214ac318ba49448a133bab2a184766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/166 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_ita.csv\n",
      "\n",
      "Processing khm: 332 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa57234844240e1b15d2476d6dbebdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4fd9735e294c45b992103a30b8b519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/332 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_khm.csv\n",
      "\n",
      "Processing mya: 144 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6b2093c5d14dc0a1bbfd5b9689bcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333d0d81762f4c3b92a833746648c238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/144 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_mya.csv\n",
      "\n",
      "Processing nep: 100 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ae914137084b70b82538dd6124ef00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e67125b262d405dbfa362be2a58df32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_nep.csv\n",
      "\n",
      "Processing ori: 118 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166bf0cf190441e79efdce6ed2bd089a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0118d759ff234f13897cbd40cf2bb6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/118 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_ori.csv\n",
      "\n",
      "Processing pan: 100 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09e811621b74e0e8985f485f5ce0ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960f8fdff9254f19919d0ce486d0a2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_pan.csv\n",
      "\n",
      "Processing pol: 119 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b954a70e32d54153b99f26ce6524efa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd325fe3c88b4b5590e7a141b935a87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/119 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_pol.csv\n",
      "\n",
      "Processing rus: 167 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63166fca620c4c3f90122626b434ec1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56aca174ac3436b9833655945cc5385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/167 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_rus.csv\n",
      "\n",
      "Processing spa: 165 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521697e262e244a69a6d07b0b64b00db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465d09b9ad024cefae19cb4ae304d8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/165 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_spa.csv\n",
      "\n",
      "Processing swa: 349 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87d0e28a3cd4355b1a800b1fd8f02d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fafd3483284d8fa3b282b18184b2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/349 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_swa.csv\n",
      "\n",
      "Processing tel: 118 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aedb15ebff2e4f4496ade3f01ddce315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b97f5d9ed845e4997c99e038fced6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/118 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclear response: 0\n",
      "Explanation:\n",
      "‡∞à ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç\n",
      "Saved predictions to predictions/subtask_1/pred_tel.csv\n",
      "\n",
      "Processing tur: 115 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cf4ad05c3249ca87d1227bc733c112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd7cf45550e4619a4396150ee802e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/115 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_tur.csv\n",
      "\n",
      "Processing urd: 177 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f68ade5caf4fccaa95da47b6f06567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fde45d79ec7457f90108f20bd149841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/177 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_urd.csv\n",
      "\n",
      "Processing zho: 214 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d7d56fe0cb4aa1bc5c1ee01d630b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835b5124d0414734802e1491c4ab84ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/214 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to predictions/subtask_1/pred_zho.csv\n",
      "CPU times: user 9.75 s, sys: 214 ms, total: 9.96 s\n",
      "Wall time: 55.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "create_submission_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d50e063-98d2-4f5f-a97b-76367dcc65e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
