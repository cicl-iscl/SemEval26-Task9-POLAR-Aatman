{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-markdown",
   "metadata": {},
   "source": [
    "# Llama Guard Few-Shot Classification with vLLM\n",
    "\n",
    "This notebook implements few-shot classification using Llama Guard 3 8B with vLLM for efficient batch inference.\n",
    "We use 20 examples from the training set (10 per class) to guide the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-uv",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-vllm",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "!uv pip install vllm --torch-backend=auto --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-vllm",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "DEV_DATA_PATH = \"../../data/dev_phase/subtask1/dev\"\n",
    "TRAIN_DATA_PATH = \"../../data/dev_phase/subtask1/train\"\n",
    "OUTPUT_DIR = \"../../predictions/llama_guard_vllm_few_shot\"\n",
    "\n",
    "# Language codes\n",
    "LANG_CODES = [\n",
    "    \"amh\",\n",
    "    \"arb\",\n",
    "    \"ben\",\n",
    "    \"deu\",\n",
    "    \"eng\",\n",
    "    \"fas\",\n",
    "    \"hau\",\n",
    "    \"hin\",\n",
    "    \"ita\",\n",
    "    \"khm\",\n",
    "    \"mya\",\n",
    "    \"nep\",\n",
    "    \"ori\",\n",
    "    \"pan\",\n",
    "    \"pol\",\n",
    "    \"rus\",\n",
    "    \"spa\",\n",
    "    \"swa\",\n",
    "    \"tel\",\n",
    "    \"tur\",\n",
    "    \"urd\",\n",
    "    \"zho\",\n",
    "]\n",
    "\n",
    "# Few-shot configuration\n",
    "N_EXAMPLES_PER_CLASS = 10  # 10 examples for label 0, 10 for label 1\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-loading-markdown",
   "metadata": {},
   "source": [
    "## Load Llama Guard Model with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "llm = LLM(\n",
    "    model=\"meta-llama/Llama-Guard-3-8B\",\n",
    "    dtype=\"auto\",\n",
    "    gpu_memory_utilization=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taxonomy-markdown",
   "metadata": {},
   "source": [
    "## Custom Polarization Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taxonomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLARIZATION_TAXONOMY = \"\"\"<BEGIN UNSAFE CONTENT CATEGORIES>\n",
    "P1: Political Polarization\n",
    "Content that creates strong us-vs-them divisions, vilifies political groups, or uses divisive rhetoric about political figures or parties.\n",
    "\n",
    "P2: Implicit Hate Speech\n",
    "Subtle expressions of prejudice, stereotyping, or derogatory characterizations of groups based on identity, religion, ethnicity, or nationality.\n",
    "\n",
    "P3: Conspiracy Theories & Misinformation\n",
    "Content promoting unfounded conspiracy theories, electoral fraud claims, or misinformation that fuels distrust in institutions.\n",
    "\n",
    "P4: Sarcasm & Mockery with Hostile Intent\n",
    "Sarcastic or mocking language directed at groups or individuals with intent to demean, ridicule, or inflame tensions.\n",
    "\n",
    "P5: Gender-Based Hostility\n",
    "Content expressing misogyny, gender-based violence, or hostile attitudes toward gender groups.\n",
    "<END UNSAFE CONTENT CATEGORIES>\n",
    "\n",
    "<BEGIN POLICY>\n",
    "Determine if the content exhibits polarization according to the categories above. Content is polarized if it contains divisive political rhetoric, conspiracy theories, implicit hate speech, hostile sarcasm, or gender-based hostility. Consider context and implied meanings, not just explicit language.\n",
    "<END POLICY>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Few-Shot Example Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_few_shot_examples(lang_code, n_per_class=10, seed=42):\n",
    "    \"\"\"Load balanced few-shot examples from training data.\"\"\"\n",
    "    train_file = os.path.join(TRAIN_DATA_PATH, f\"{lang_code}.csv\")\n",
    "\n",
    "    if not os.path.exists(train_file):\n",
    "        raise FileNotFoundError(f\"No training file for {lang_code}\")\n",
    "\n",
    "    df = pd.read_csv(train_file)\n",
    "\n",
    "    # Sample n_per_class examples from each class\n",
    "    polarized = df[df[\"polarization\"] == 1].sample(\n",
    "        n=n_per_class, random_state=seed\n",
    "    )\n",
    "    non_polarized = df[df[\"polarization\"] == 0].sample(\n",
    "        n=n_per_class, random_state=seed\n",
    "    )\n",
    "\n",
    "    # Combine and shuffle\n",
    "    return pd.concat([polarized, non_polarized]).sample(frac=1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_few_shot_prefix(lang_code, few_shot_df):\n",
    "    \"\"\"Build the few-shot prompt prefix with examples.\"\"\"\n",
    "    header = f\"\"\"{POLARIZATION_TAXONOMY}\n",
    "\n",
    "Language: {lang_code}\n",
    "\n",
    "Below are labeled examples:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    examples = []\n",
    "    for _, row in few_shot_df.iterrows():\n",
    "        # Use \"unsafe\" for polarized (1) and \"safe\" for non-polarized (0)\n",
    "        label = \"unsafe\" if row[\"polarization\"] == 1 else \"safe\"\n",
    "        examples.append(\n",
    "            f\"Text: {row['text']}\\nClassification: {label}\\n\"\n",
    "        )\n",
    "\n",
    "    examples_block = \"\\n\".join(examples)\n",
    "\n",
    "    footer = \"\"\"\n",
    "Now classify the following text:\n",
    "\n",
    "Text: {text}\n",
    "Classification:\"\"\"\n",
    "\n",
    "    return header + examples_block + footer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cache-prefixes-markdown",
   "metadata": {},
   "source": [
    "## Cache Few-Shot Prefixes for All Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cache-prefixes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache Prefixes Per Language\n",
    "FEW_SHOT_PREFIXES = {}\n",
    "\n",
    "for lang in LANG_CODES:\n",
    "    try:\n",
    "        few_shot_df = load_few_shot_examples(lang, n_per_class=N_EXAMPLES_PER_CLASS, seed=RANDOM_SEED)\n",
    "        FEW_SHOT_PREFIXES[lang] = build_few_shot_prefix(lang, few_shot_df)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Skipping {lang}: {e}\")\n",
    "\n",
    "print(f\"✓ Few-shot prefixes ready for {len(FEW_SHOT_PREFIXES)} languages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-prefix-markdown",
   "metadata": {},
   "source": [
    "## Test Few-Shot Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample prefix\n",
    "test_lang = \"eng\"\n",
    "if test_lang in FEW_SHOT_PREFIXES:\n",
    "    sample_prefix = FEW_SHOT_PREFIXES[test_lang].format(text=\"SAMPLE_TEXT_HERE\")\n",
    "    print(f\"Sample prefix for {test_lang}:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(sample_prefix[:1000] + \"...\" if len(sample_prefix) > 1000 else sample_prefix)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-creation-markdown",
   "metadata": {},
   "source": [
    "## Prompt Creation and Response Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(text, language_code):\n",
    "    \"\"\"Create a few-shot prompt for polarization detection.\"\"\"\n",
    "    prefix = FEW_SHOT_PREFIXES[language_code]\n",
    "    return prefix.format(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parse-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response):\n",
    "    \"\"\"\n",
    "    Parse Llama Guard response to binary label.\n",
    "    \n",
    "    Llama Guard returns:\n",
    "    - \"safe\" or \"safe\\n\" for non-polarized content (label 0)\n",
    "    - \"unsafe\" or \"\\n\\nunsafe\\nS1\" for polarized content (label 1)\n",
    "    \"\"\"\n",
    "    if response is None:\n",
    "        return 0  # Default to non-polarized if error\n",
    "\n",
    "    response_lower = response.lower().strip()\n",
    "\n",
    "    # Check for \"unsafe\" first (polarized = 1)\n",
    "    if \"unsafe\" in response_lower:\n",
    "        return 1\n",
    "    # Check for \"safe\" (non-polarized = 0)\n",
    "    elif \"safe\" in response_lower:\n",
    "        return 0\n",
    "    else:\n",
    "        print(f\"Unclear response: {response}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-inference-markdown",
   "metadata": {},
   "source": [
    "## Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test with sample texts\n",
    "test_text_1 = \"I hate all politicians, they are destroying our country!\"\n",
    "test_text_2 = \"The weather is nice today.\"\n",
    "\n",
    "test_prompts = [\n",
    "    create_prompt(test_text_1, \"eng\"),\n",
    "    create_prompt(test_text_2, \"eng\")\n",
    "]\n",
    "\n",
    "test_params = SamplingParams(\n",
    "    temperature=0.0,  # Deterministic\n",
    "    max_tokens=10,    # Short responses\n",
    ")\n",
    "\n",
    "test_outputs = llm.generate(test_prompts, test_params)\n",
    "\n",
    "for i, output in enumerate(test_outputs):\n",
    "    response = output.outputs[0].text.strip()\n",
    "    prediction = parse_response(response)\n",
    "    test_text = test_text_1 if i == 0 else test_text_2\n",
    "    print(f\"Text: {test_text}\")\n",
    "    print(f\"Raw response: {response}\")\n",
    "    print(f\"Predicted label: {prediction}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "process-dev-markdown",
   "metadata": {},
   "source": [
    "## Process Dev Set with Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_language(lang_code):\n",
    "    \"\"\"Process one language file with batched inference\"\"\"\n",
    "    input_file = os.path.join(DEV_DATA_PATH, f\"{lang_code}.csv\")\n",
    "\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"File not found: {input_file}\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"\\nProcessing {lang_code}: {len(df)} samples\")\n",
    "\n",
    "    # Prepare all prompts as a list\n",
    "    prompts = [create_prompt(row[\"text\"], lang_code) for _, row in df.iterrows()]\n",
    "\n",
    "    # Define sampling params for fast, short outputs\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.0,  # Deterministic\n",
    "        max_tokens=10,    # Enough for \"safe\" or \"unsafe\\nS1\"\n",
    "    )\n",
    "\n",
    "    # Batched generation - vLLM handles batching automatically\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    # Extract and parse responses\n",
    "    predictions = []\n",
    "    for idx, output in enumerate(outputs):\n",
    "        response = output.outputs[0].text.strip()  # Get the generated text\n",
    "        prediction = parse_response(response)\n",
    "        post_id = df.iloc[idx][\"id\"]\n",
    "        predictions.append({\"id\": post_id, \"polarization\": prediction})\n",
    "\n",
    "    return pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_zip(lang_codes_to_process=None):\n",
    "    \"\"\"Create submission zip file\"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    if lang_codes_to_process is None:\n",
    "        lang_codes_to_process = LANG_CODES\n",
    "\n",
    "    for lang_code in lang_codes_to_process:\n",
    "        pred_df = process_language(lang_code)\n",
    "\n",
    "        if pred_df is not None:\n",
    "            output_file = os.path.join(OUTPUT_DIR, f\"pred_{lang_code}.csv\")\n",
    "            pred_df.to_csv(output_file, index=False)\n",
    "            print(f\"Saved predictions to {output_file}\")\n",
    "\n",
    "    zip_filename = \"subtask_1_llama_guard_vllm.zip\"\n",
    "    with zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file in os.listdir(OUTPUT_DIR):\n",
    "            if file.startswith(\"pred_\") and file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(OUTPUT_DIR, file)\n",
    "                zipf.write(file_path, os.path.join(\"subtask_1\", file))\n",
    "    \n",
    "    print(f\"\\n✓ Created submission zip: {zip_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-markdown",
   "metadata": {},
   "source": [
    "## Run Predictions\n",
    "\n",
    "**Note:** Start with a single language to test, then process all languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single language for testing\n",
    "create_submission_zip([\"eng\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to process all languages\n",
    "# %%time\n",
    "# create_submission_zip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-markdown",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View predictions for a language\n",
    "sample_lang = \"eng\"\n",
    "pred_file = os.path.join(OUTPUT_DIR, f\"pred_{sample_lang}.csv\")\n",
    "\n",
    "if os.path.exists(pred_file):\n",
    "    pred_df = pd.read_csv(pred_file)\n",
    "    print(f\"Predictions for {sample_lang}:\")\n",
    "    print(f\"Total samples: {len(pred_df)}\")\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    print(pred_df[\"polarization\"].value_counts())\n",
    "    print(f\"\\nFirst 10 predictions:\")\n",
    "    print(pred_df.head(10))\n",
    "else:\n",
    "    print(f\"No predictions found for {sample_lang}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
