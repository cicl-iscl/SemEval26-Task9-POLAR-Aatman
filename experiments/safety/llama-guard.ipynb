{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90e7454-a3da-457d-9402-5b39a36eb5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.1 ms, sys: 4.07 ms, total: 12.2 ms\n",
      "Wall time: 7.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "!pip install --upgrade transformers\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header-markdown",
   "metadata": {},
   "source": [
    "# Llama Guard Few-Shot Classification for Polarization Detection\n",
    "\n",
    "This notebook implements few-shot classification using Llama Guard 3 8B model.\n",
    "We use 20 examples from the training set (10 per class) to guide the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "703e6b01-08ca-428d-acda-b08cfb4a27af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba9d2d5-400d-4bf2-b129-86976ef4acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-Guard-3-8B\"\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97345486-9d3c-4c62-b5df-9ed8bdcb9a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e486521b91f24037aae7d38da3de6c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4115ed139147fe9391c998021c98c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b091bebe58154b11a71450cd5cf507e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc3a6bd97d744338298a8d7dddcbd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/860 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d489dfe301c4400bc158ccfe10291e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a530b9097424348b18b94de4d4d6344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34249d4da9d14725a1e4445c3310a2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2ad91da2154b7d99483daf97b1b1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abda824eb104499a19591cbf351f262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1a0f9ec9f14c7c85ea893735933187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186edabe25f24d96b181b4033a35cfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d3dbd4c302400788ce607676bbe4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 s, sys: 17.8 s, total: 39.3 s\n",
      "Wall time: 44.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# hf auth login\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, dtype=dtype, device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "config-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "DEV_DATA_PATH = \"subtask1/dev\"\n",
    "TRAIN_DATA_PATH = \"subtask1/train\"\n",
    "OUTPUT_DIR = \"./llama_guard_few_shot\"\n",
    "\n",
    "# Language codes\n",
    "LANG_CODES = [\n",
    "    \"amh\", \"arb\", \"ben\", \"deu\", \"eng\", \"fas\", \"hau\", \"hin\",\n",
    "    \"ita\", \"khm\", \"mya\", \"nep\", \"ori\", \"pan\", \"pol\", \"rus\",\n",
    "    \"spa\", \"swa\", \"tel\", \"tur\", \"urd\", \"zho\"\n",
    "]\n",
    "\n",
    "# Few-shot configuration\n",
    "N_EXAMPLES_PER_CLASS = 10  # 10 examples for label 0, 10 for label 1\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taxonomy-markdown",
   "metadata": {},
   "source": [
    "## Custom Polarization Taxonomy for Llama Guard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "taxonomy-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLARIZATION_TAXONOMY = \"\"\"<BEGIN UNSAFE CONTENT CATEGORIES>\n",
    "P1: Political Polarization\n",
    "Content that creates strong us-vs-them divisions, vilifies political groups, or uses divisive rhetoric about political figures or parties.\n",
    "\n",
    "P2: Implicit Hate Speech\n",
    "Subtle expressions of prejudice, stereotyping, or derogatory characterizations of groups based on identity, religion, ethnicity, or nationality.\n",
    "\n",
    "P3: Conspiracy Theories & Misinformation\n",
    "Content promoting unfounded conspiracy theories, electoral fraud claims, or misinformation that fuels distrust in institutions.\n",
    "\n",
    "P4: Sarcasm & Mockery with Hostile Intent\n",
    "Sarcastic or mocking language directed at groups or individuals with intent to demean, ridicule, or inflame tensions.\n",
    "\n",
    "P5: Gender-Based Hostility\n",
    "Content expressing misogyny, gender-based violence, or hostile attitudes toward gender groups.\n",
    "<END UNSAFE CONTENT CATEGORIES>\n",
    "\n",
    "<BEGIN POLICY>\n",
    "Determine if the content exhibits polarization according to the categories above. Content is polarized if it contains divisive political rhetoric, conspiracy theories, implicit hate speech, hostile sarcasm, or gender-based hostility. Consider context and implied meanings, not just explicit language.\n",
    "<END POLICY>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "load-examples-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_few_shot_examples(lang_code, n_per_class=10, seed=42):\n",
    "    \"\"\"Load balanced few-shot examples from training data.\"\"\"\n",
    "    train_file = os.path.join(TRAIN_DATA_PATH, f\"{lang_code}.csv\")\n",
    "    \n",
    "    if not os.path.exists(train_file):\n",
    "        raise FileNotFoundError(f\"No training file for {lang_code}\")\n",
    "    \n",
    "    df = pd.read_csv(train_file)\n",
    "    \n",
    "    # Sample n_per_class examples from each class\n",
    "    polarized = df[df[\"polarization\"] == 1].sample(n=n_per_class, random_state=seed)\n",
    "    non_polarized = df[df[\"polarization\"] == 0].sample(n=n_per_class, random_state=seed)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    examples = pd.concat([polarized, non_polarized]).sample(frac=1, random_state=seed)\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-building-markdown",
   "metadata": {},
   "source": [
    "## Prompt Building with Few-Shot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "build-prompt-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_few_shot_prompt(text, lang_code, few_shot_examples):\n",
    "    \"\"\"\n",
    "    Build a Llama Guard prompt with custom taxonomy and few-shot examples.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to classify\n",
    "        lang_code: ISO 639 language code\n",
    "        few_shot_examples: DataFrame with 'text' and 'polarization' columns\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    # Build examples section\n",
    "    examples_text = \"\\n\\nHere are some labeled examples:\\n\\n\"\n",
    "    \n",
    "    for idx, row in few_shot_examples.iterrows():\n",
    "        label = \"unsafe\" if row[\"polarization\"] == 1 else \"safe\"\n",
    "        examples_text += f\"Text: {row['text']}\\n\"\n",
    "        examples_text += f\"Classification: {label}\\n\\n\"\n",
    "    \n",
    "    # Build the full prompt\n",
    "    prompt = f\"\"\"{POLARIZATION_TAXONOMY}\n",
    "\n",
    "Language: {lang_code}\n",
    "{examples_text}\n",
    "Now classify the following text:\n",
    "\n",
    "Text: {text}\n",
    "Classification:\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-markdown",
   "metadata": {},
   "source": [
    "## Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moderate-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moderate(chat):\n",
    "    \"\"\"Generate response from Llama Guard.\"\"\"\n",
    "    input_ids = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(input_ids=input_ids, max_new_tokens=100, pad_token_id=0)\n",
    "    prompt_len = input_ids.shape[-1]\n",
    "    \n",
    "    return tokenizer.decode(output[0][prompt_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "parse-response-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_llama_guard_response(response):\n",
    "    \"\"\"\n",
    "    Parse Llama Guard response to binary label.\n",
    "    \n",
    "    Llama Guard returns:\n",
    "    - \"safe\" or \"safe\\n\" for non-polarized content (label 0)\n",
    "    - \"unsafe\\nS1\" or similar for polarized content (label 1)\n",
    "    \n",
    "    Args:\n",
    "        response: Raw response from Llama Guard\n",
    "    \n",
    "    Returns:\n",
    "        0 for safe (non-polarized), 1 for unsafe (polarized)\n",
    "    \"\"\"\n",
    "    if response is None:\n",
    "        return 0  # Default to non-polarized\n",
    "    \n",
    "    response_lower = response.lower().strip()\n",
    "    \n",
    "    # Check for \"unsafe\" first (polarized = 1)\n",
    "    if \"unsafe\" in response_lower:\n",
    "        return 1\n",
    "    # Check for \"safe\" (non-polarized = 0)\n",
    "    elif \"safe\" in response_lower:\n",
    "        return 0\n",
    "    else:\n",
    "        # If unclear, default to non-polarized\n",
    "        print(f\"Warning: Unclear response: {response}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "classify-text-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, lang_code, few_shot_examples):\n",
    "    \"\"\"\n",
    "    Classify a single text using Llama Guard with few-shot examples.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to classify\n",
    "        lang_code: Language code\n",
    "        few_shot_examples: DataFrame with training examples\n",
    "    \n",
    "    Returns:\n",
    "        Binary label (0 or 1)\n",
    "    \"\"\"\n",
    "    # Build prompt with few-shot examples\n",
    "    prompt = build_few_shot_prompt(text, lang_code, few_shot_examples)\n",
    "    \n",
    "    # Format as chat for Llama Guard\n",
    "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Generate response\n",
    "    response = moderate(chat)\n",
    "    \n",
    "    # Parse response to binary label\n",
    "    label = parse_llama_guard_response(response)\n",
    "    \n",
    "    return label, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-markdown",
   "metadata": {},
   "source": [
    "## Test the Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "test-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 few-shot examples for eng\n",
      "\n",
      "Class distribution:\n",
      "polarization\n",
      "1    10\n",
      "0    10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Test with a simple example\n",
    "test_lang = \"eng\"\n",
    "test_examples = load_few_shot_examples(test_lang, n_per_class=N_EXAMPLES_PER_CLASS, seed=RANDOM_SEED)\n",
    "\n",
    "print(f\"Loaded {len(test_examples)} few-shot examples for {test_lang}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(test_examples[\"polarization\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "test-classify-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 (polarized):\n",
      "Text: I hate all politicians, they are destroying our country!\n",
      "Raw response: \n",
      "\n",
      "safe\n",
      "Predicted label: 0\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Test 2 (non-polarized):\n",
      "Text: The weather is nice today.\n",
      "Raw response: \n",
      "\n",
      "safe\n",
      "Predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "# Test classification\n",
    "test_text_1 = \"I hate all politicians, they are destroying our country!\"\n",
    "test_text_2 = \"The weather is nice today.\"\n",
    "\n",
    "print(\"Test 1 (polarized):\")\n",
    "label_1, response_1 = classify_text(test_text_1, test_lang, test_examples)\n",
    "print(f\"Text: {test_text_1}\")\n",
    "print(f\"Raw response: {response_1}\")\n",
    "print(f\"Predicted label: {label_1}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Test 2 (non-polarized):\")\n",
    "label_2, response_2 = classify_text(test_text_2, test_lang, test_examples)\n",
    "print(f\"Text: {test_text_2}\")\n",
    "print(f\"Raw response: {response_2}\")\n",
    "print(f\"Predicted label: {label_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "process-dev-markdown",
   "metadata": {},
   "source": [
    "## Process Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "process-language-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_language(lang_code, few_shot_examples):\n",
    "    \"\"\"\n",
    "    Process all samples for a single language.\n",
    "    \n",
    "    Args:\n",
    "        lang_code: Language code to process\n",
    "        few_shot_examples: DataFrame with training examples for this language\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with predictions\n",
    "    \"\"\"\n",
    "    dev_file = os.path.join(DEV_DATA_PATH, f\"{lang_code}.csv\")\n",
    "    \n",
    "    if not os.path.exists(dev_file):\n",
    "        print(f\"Dev file not found: {dev_file}\")\n",
    "        return None\n",
    "    \n",
    "    # Load dev data\n",
    "    dev_df = pd.read_csv(dev_file)\n",
    "    print(f\"\\nProcessing {lang_code}: {len(dev_df)} samples\")\n",
    "    \n",
    "    # Process each sample\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in dev_df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        post_id = row[\"id\"]\n",
    "        \n",
    "        # Classify\n",
    "        label, response = classify_text(text, lang_code, few_shot_examples)\n",
    "        \n",
    "        predictions.append({\n",
    "            \"id\": post_id,\n",
    "            \"polarization\": label\n",
    "        })\n",
    "        \n",
    "        # # Progress indicator\n",
    "        # if (idx + 1) % 10 == 0:\n",
    "        #     print(f\"  Processed {idx + 1}/{len(dev_df)} samples\")\n",
    "    \n",
    "    return pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "process-all-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_languages(lang_codes=None):\n",
    "    \"\"\"\n",
    "    Process all languages and save predictions.\n",
    "    \n",
    "    Args:\n",
    "        lang_codes: List of language codes to process (default: all)\n",
    "    \"\"\"\n",
    "    if lang_codes is None:\n",
    "        lang_codes = LANG_CODES\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Process each language\n",
    "    for lang_code in lang_codes:\n",
    "        try:\n",
    "            # Load few-shot examples for this language\n",
    "            few_shot_examples = load_few_shot_examples(\n",
    "                lang_code, \n",
    "                n_per_class=N_EXAMPLES_PER_CLASS, \n",
    "                seed=RANDOM_SEED\n",
    "            )\n",
    "            \n",
    "            # Process dev set\n",
    "            pred_df = process_language(lang_code, few_shot_examples)\n",
    "            \n",
    "            if pred_df is not None:\n",
    "                # Save predictions\n",
    "                output_file = os.path.join(OUTPUT_DIR, f\"pred_{lang_code}.csv\")\n",
    "                pred_df.to_csv(output_file, index=False)\n",
    "                print(f\"✓ Saved predictions to {output_file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {lang_code}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-markdown",
   "metadata": {},
   "source": [
    "## Run Predictions on Dev Set\n",
    "\n",
    "**Note:** You can process a single language first to test, or process all languages.\n",
    "Processing all languages may take a while depending on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "run-single-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single language for testing\n",
    "# process_all_languages([\"eng\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "run-all-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing amh: 166 samples\n",
      "  Processed 10/166 samples\n",
      "  Processed 20/166 samples\n",
      "  Processed 30/166 samples\n",
      "  Processed 40/166 samples\n",
      "  Processed 50/166 samples\n",
      "  Processed 60/166 samples\n",
      "  Processed 70/166 samples\n",
      "  Processed 80/166 samples\n",
      "  Processed 90/166 samples\n",
      "  Processed 100/166 samples\n",
      "  Processed 110/166 samples\n",
      "  Processed 120/166 samples\n",
      "  Processed 130/166 samples\n",
      "  Processed 140/166 samples\n",
      "  Processed 150/166 samples\n",
      "  Processed 160/166 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_amh.csv\n",
      "\n",
      "Processing arb: 169 samples\n",
      "  Processed 10/169 samples\n",
      "  Processed 20/169 samples\n",
      "  Processed 30/169 samples\n",
      "  Processed 40/169 samples\n",
      "  Processed 50/169 samples\n",
      "  Processed 60/169 samples\n",
      "  Processed 70/169 samples\n",
      "  Processed 80/169 samples\n",
      "  Processed 90/169 samples\n",
      "  Processed 100/169 samples\n",
      "  Processed 110/169 samples\n",
      "  Processed 120/169 samples\n",
      "  Processed 130/169 samples\n",
      "  Processed 140/169 samples\n",
      "  Processed 150/169 samples\n",
      "  Processed 160/169 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_arb.csv\n",
      "\n",
      "Processing ben: 166 samples\n",
      "  Processed 10/166 samples\n",
      "  Processed 20/166 samples\n",
      "  Processed 30/166 samples\n",
      "  Processed 40/166 samples\n",
      "  Processed 50/166 samples\n",
      "  Processed 60/166 samples\n",
      "  Processed 70/166 samples\n",
      "  Processed 80/166 samples\n",
      "  Processed 90/166 samples\n",
      "  Processed 100/166 samples\n",
      "  Processed 110/166 samples\n",
      "  Processed 120/166 samples\n",
      "  Processed 130/166 samples\n",
      "  Processed 140/166 samples\n",
      "  Processed 150/166 samples\n",
      "  Processed 160/166 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_ben.csv\n",
      "\n",
      "Processing deu: 159 samples\n",
      "  Processed 10/159 samples\n",
      "  Processed 20/159 samples\n",
      "  Processed 30/159 samples\n",
      "  Processed 40/159 samples\n",
      "  Processed 50/159 samples\n",
      "  Processed 60/159 samples\n",
      "  Processed 70/159 samples\n",
      "  Processed 80/159 samples\n",
      "  Processed 90/159 samples\n",
      "  Processed 100/159 samples\n",
      "  Processed 110/159 samples\n",
      "  Processed 120/159 samples\n",
      "  Processed 130/159 samples\n",
      "  Processed 140/159 samples\n",
      "  Processed 150/159 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_deu.csv\n",
      "\n",
      "Processing eng: 160 samples\n",
      "  Processed 10/160 samples\n",
      "  Processed 20/160 samples\n",
      "  Processed 30/160 samples\n",
      "  Processed 40/160 samples\n",
      "  Processed 50/160 samples\n",
      "  Processed 60/160 samples\n",
      "  Processed 70/160 samples\n",
      "  Processed 80/160 samples\n",
      "  Processed 90/160 samples\n",
      "  Processed 100/160 samples\n",
      "  Processed 110/160 samples\n",
      "  Processed 120/160 samples\n",
      "  Processed 130/160 samples\n",
      "  Processed 140/160 samples\n",
      "  Processed 150/160 samples\n",
      "  Processed 160/160 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_eng.csv\n",
      "\n",
      "Processing fas: 164 samples\n",
      "  Processed 10/164 samples\n",
      "  Processed 20/164 samples\n",
      "  Processed 30/164 samples\n",
      "  Processed 40/164 samples\n",
      "  Processed 50/164 samples\n",
      "  Processed 60/164 samples\n",
      "  Processed 70/164 samples\n",
      "  Processed 80/164 samples\n",
      "  Processed 90/164 samples\n",
      "  Processed 100/164 samples\n",
      "  Processed 110/164 samples\n",
      "  Processed 120/164 samples\n",
      "  Processed 130/164 samples\n",
      "  Processed 140/164 samples\n",
      "  Processed 150/164 samples\n",
      "  Processed 160/164 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_fas.csv\n",
      "\n",
      "Processing hau: 182 samples\n",
      "  Processed 10/182 samples\n",
      "  Processed 20/182 samples\n",
      "  Processed 30/182 samples\n",
      "  Processed 40/182 samples\n",
      "  Processed 50/182 samples\n",
      "  Processed 60/182 samples\n",
      "  Processed 70/182 samples\n",
      "  Processed 80/182 samples\n",
      "  Processed 90/182 samples\n",
      "  Processed 100/182 samples\n",
      "  Processed 110/182 samples\n",
      "  Processed 120/182 samples\n",
      "  Processed 130/182 samples\n",
      "  Processed 140/182 samples\n",
      "  Processed 150/182 samples\n",
      "  Processed 160/182 samples\n",
      "  Processed 170/182 samples\n",
      "  Processed 180/182 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_hau.csv\n",
      "\n",
      "Processing hin: 137 samples\n",
      "  Processed 10/137 samples\n",
      "  Processed 20/137 samples\n",
      "  Processed 30/137 samples\n",
      "  Processed 40/137 samples\n",
      "  Processed 50/137 samples\n",
      "  Processed 60/137 samples\n",
      "  Processed 70/137 samples\n",
      "  Processed 80/137 samples\n",
      "  Processed 90/137 samples\n",
      "  Processed 100/137 samples\n",
      "  Processed 110/137 samples\n",
      "  Processed 120/137 samples\n",
      "  Processed 130/137 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_hin.csv\n",
      "\n",
      "Processing ita: 166 samples\n",
      "  Processed 10/166 samples\n",
      "  Processed 20/166 samples\n",
      "  Processed 30/166 samples\n",
      "  Processed 40/166 samples\n",
      "  Processed 50/166 samples\n",
      "  Processed 60/166 samples\n",
      "  Processed 70/166 samples\n",
      "  Processed 80/166 samples\n",
      "  Processed 90/166 samples\n",
      "  Processed 100/166 samples\n",
      "  Processed 110/166 samples\n",
      "  Processed 120/166 samples\n",
      "  Processed 130/166 samples\n",
      "  Processed 140/166 samples\n",
      "  Processed 150/166 samples\n",
      "  Processed 160/166 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_ita.csv\n",
      "\n",
      "Processing khm: 332 samples\n",
      "  Processed 10/332 samples\n",
      "  Processed 20/332 samples\n",
      "  Processed 30/332 samples\n",
      "  Processed 40/332 samples\n",
      "  Processed 50/332 samples\n",
      "  Processed 60/332 samples\n",
      "  Processed 70/332 samples\n",
      "  Processed 80/332 samples\n",
      "  Processed 90/332 samples\n",
      "  Processed 100/332 samples\n",
      "  Processed 110/332 samples\n",
      "  Processed 120/332 samples\n",
      "  Processed 130/332 samples\n",
      "  Processed 140/332 samples\n",
      "  Processed 150/332 samples\n",
      "  Processed 160/332 samples\n",
      "  Processed 170/332 samples\n",
      "  Processed 180/332 samples\n",
      "  Processed 190/332 samples\n",
      "  Processed 200/332 samples\n",
      "  Processed 210/332 samples\n",
      "  Processed 220/332 samples\n",
      "  Processed 230/332 samples\n",
      "  Processed 240/332 samples\n",
      "  Processed 250/332 samples\n",
      "  Processed 260/332 samples\n",
      "  Processed 270/332 samples\n",
      "  Processed 280/332 samples\n",
      "  Processed 290/332 samples\n",
      "  Processed 300/332 samples\n",
      "  Processed 310/332 samples\n",
      "  Processed 320/332 samples\n",
      "  Processed 330/332 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_khm.csv\n",
      "\n",
      "Processing mya: 144 samples\n",
      "  Processed 10/144 samples\n",
      "  Processed 20/144 samples\n",
      "  Processed 30/144 samples\n",
      "  Processed 40/144 samples\n",
      "  Processed 50/144 samples\n",
      "  Processed 60/144 samples\n",
      "  Processed 70/144 samples\n",
      "  Processed 80/144 samples\n",
      "  Processed 90/144 samples\n",
      "  Processed 100/144 samples\n",
      "  Processed 110/144 samples\n",
      "  Processed 120/144 samples\n",
      "  Processed 130/144 samples\n",
      "  Processed 140/144 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_mya.csv\n",
      "\n",
      "Processing nep: 100 samples\n",
      "  Processed 10/100 samples\n",
      "  Processed 20/100 samples\n",
      "  Processed 30/100 samples\n",
      "  Processed 40/100 samples\n",
      "  Processed 50/100 samples\n",
      "  Processed 60/100 samples\n",
      "  Processed 70/100 samples\n",
      "  Processed 80/100 samples\n",
      "  Processed 90/100 samples\n",
      "  Processed 100/100 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_nep.csv\n",
      "\n",
      "Processing ori: 118 samples\n",
      "  Processed 10/118 samples\n",
      "  Processed 20/118 samples\n",
      "  Processed 30/118 samples\n",
      "  Processed 40/118 samples\n",
      "  Processed 50/118 samples\n",
      "  Processed 60/118 samples\n",
      "  Processed 70/118 samples\n",
      "  Processed 80/118 samples\n",
      "  Processed 90/118 samples\n",
      "  Processed 100/118 samples\n",
      "  Processed 110/118 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_ori.csv\n",
      "\n",
      "Processing pan: 100 samples\n",
      "  Processed 10/100 samples\n",
      "  Processed 20/100 samples\n",
      "  Processed 30/100 samples\n",
      "  Processed 40/100 samples\n",
      "  Processed 50/100 samples\n",
      "  Processed 60/100 samples\n",
      "  Processed 70/100 samples\n",
      "  Processed 80/100 samples\n",
      "  Processed 90/100 samples\n",
      "  Processed 100/100 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_pan.csv\n",
      "\n",
      "Processing pol: 119 samples\n",
      "  Processed 10/119 samples\n",
      "  Processed 20/119 samples\n",
      "  Processed 30/119 samples\n",
      "  Processed 40/119 samples\n",
      "  Processed 50/119 samples\n",
      "  Processed 60/119 samples\n",
      "  Processed 70/119 samples\n",
      "  Processed 80/119 samples\n",
      "  Processed 90/119 samples\n",
      "  Processed 100/119 samples\n",
      "  Processed 110/119 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_pol.csv\n",
      "\n",
      "Processing rus: 167 samples\n",
      "  Processed 10/167 samples\n",
      "  Processed 20/167 samples\n",
      "  Processed 30/167 samples\n",
      "  Processed 40/167 samples\n",
      "  Processed 50/167 samples\n",
      "  Processed 60/167 samples\n",
      "  Processed 70/167 samples\n",
      "  Processed 80/167 samples\n",
      "  Processed 90/167 samples\n",
      "  Processed 100/167 samples\n",
      "  Processed 110/167 samples\n",
      "  Processed 120/167 samples\n",
      "  Processed 130/167 samples\n",
      "  Processed 140/167 samples\n",
      "  Processed 150/167 samples\n",
      "  Processed 160/167 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_rus.csv\n",
      "\n",
      "Processing spa: 165 samples\n",
      "  Processed 10/165 samples\n",
      "  Processed 20/165 samples\n",
      "  Processed 30/165 samples\n",
      "  Processed 40/165 samples\n",
      "  Processed 50/165 samples\n",
      "  Processed 60/165 samples\n",
      "  Processed 70/165 samples\n",
      "  Processed 80/165 samples\n",
      "  Processed 90/165 samples\n",
      "  Processed 100/165 samples\n",
      "  Processed 110/165 samples\n",
      "  Processed 120/165 samples\n",
      "  Processed 130/165 samples\n",
      "  Processed 140/165 samples\n",
      "  Processed 150/165 samples\n",
      "  Processed 160/165 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_spa.csv\n",
      "\n",
      "Processing swa: 349 samples\n",
      "  Processed 10/349 samples\n",
      "  Processed 20/349 samples\n",
      "  Processed 30/349 samples\n",
      "  Processed 40/349 samples\n",
      "  Processed 50/349 samples\n",
      "  Processed 60/349 samples\n",
      "  Processed 70/349 samples\n",
      "  Processed 80/349 samples\n",
      "  Processed 90/349 samples\n",
      "  Processed 100/349 samples\n",
      "  Processed 110/349 samples\n",
      "  Processed 120/349 samples\n",
      "  Processed 130/349 samples\n",
      "  Processed 140/349 samples\n",
      "  Processed 150/349 samples\n",
      "  Processed 160/349 samples\n",
      "  Processed 170/349 samples\n",
      "  Processed 180/349 samples\n",
      "  Processed 190/349 samples\n",
      "  Processed 200/349 samples\n",
      "  Processed 210/349 samples\n",
      "  Processed 220/349 samples\n",
      "  Processed 230/349 samples\n",
      "  Processed 240/349 samples\n",
      "  Processed 250/349 samples\n",
      "  Processed 260/349 samples\n",
      "  Processed 270/349 samples\n",
      "  Processed 280/349 samples\n",
      "  Processed 290/349 samples\n",
      "  Processed 300/349 samples\n",
      "  Processed 310/349 samples\n",
      "  Processed 320/349 samples\n",
      "  Processed 330/349 samples\n",
      "  Processed 340/349 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_swa.csv\n",
      "\n",
      "Processing tel: 118 samples\n",
      "  Processed 10/118 samples\n",
      "  Processed 20/118 samples\n",
      "  Processed 30/118 samples\n",
      "  Processed 40/118 samples\n",
      "  Processed 50/118 samples\n",
      "  Processed 60/118 samples\n",
      "  Processed 70/118 samples\n",
      "  Processed 80/118 samples\n",
      "  Processed 90/118 samples\n",
      "  Processed 100/118 samples\n",
      "  Processed 110/118 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_tel.csv\n",
      "\n",
      "Processing tur: 115 samples\n",
      "  Processed 10/115 samples\n",
      "  Processed 20/115 samples\n",
      "  Processed 30/115 samples\n",
      "  Processed 40/115 samples\n",
      "  Processed 50/115 samples\n",
      "  Processed 60/115 samples\n",
      "  Processed 70/115 samples\n",
      "  Processed 80/115 samples\n",
      "  Processed 90/115 samples\n",
      "  Processed 100/115 samples\n",
      "  Processed 110/115 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_tur.csv\n",
      "\n",
      "Processing urd: 177 samples\n",
      "  Processed 10/177 samples\n",
      "  Processed 20/177 samples\n",
      "  Processed 30/177 samples\n",
      "  Processed 40/177 samples\n",
      "  Processed 50/177 samples\n",
      "  Processed 60/177 samples\n",
      "  Processed 70/177 samples\n",
      "  Processed 80/177 samples\n",
      "  Processed 90/177 samples\n",
      "  Processed 100/177 samples\n",
      "  Processed 110/177 samples\n",
      "  Processed 120/177 samples\n",
      "  Processed 130/177 samples\n",
      "  Processed 140/177 samples\n",
      "  Processed 150/177 samples\n",
      "  Processed 160/177 samples\n",
      "  Processed 170/177 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_urd.csv\n",
      "\n",
      "Processing zho: 214 samples\n",
      "  Processed 10/214 samples\n",
      "  Processed 20/214 samples\n",
      "  Processed 30/214 samples\n",
      "  Processed 40/214 samples\n",
      "  Processed 50/214 samples\n",
      "  Processed 60/214 samples\n",
      "  Processed 70/214 samples\n",
      "  Processed 80/214 samples\n",
      "  Processed 90/214 samples\n",
      "  Processed 100/214 samples\n",
      "  Processed 110/214 samples\n",
      "  Processed 120/214 samples\n",
      "  Processed 130/214 samples\n",
      "  Processed 140/214 samples\n",
      "  Processed 150/214 samples\n",
      "  Processed 160/214 samples\n",
      "  Processed 170/214 samples\n",
      "  Processed 180/214 samples\n",
      "  Processed 190/214 samples\n",
      "  Processed 200/214 samples\n",
      "  Processed 210/214 samples\n",
      "✓ Saved predictions to ./llama_guard_few_shot/pred_zho.csv\n",
      "\n",
      "================================================================================\n",
      "Processing complete!\n",
      "CPU times: user 18min 9s, sys: 2min 8s, total: 20min 17s\n",
      "Wall time: 20min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Uncomment to process all languages\n",
    "process_all_languages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-markdown",
   "metadata": {},
   "source": [
    "## View Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "view-results-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View predictions for a language\n",
    "# sample_lang = \"eng\"\n",
    "# pred_file = os.path.join(OUTPUT_DIR, f\"pred_{sample_lang}.csv\")\n",
    "\n",
    "# if os.path.exists(pred_file):\n",
    "#     pred_df = pd.read_csv(pred_file)\n",
    "#     print(f\"Predictions for {sample_lang}:\")\n",
    "#     print(f\"Total samples: {len(pred_df)}\")\n",
    "#     print(f\"\\nLabel distribution:\")\n",
    "#     print(pred_df[\"polarization\"].value_counts())\n",
    "#     print(f\"\\nFirst 10 predictions:\")\n",
    "#     print(pred_df.head(10))\n",
    "# else:\n",
    "#     print(f\"No predictions found for {sample_lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1fbdf88-cacb-40ba-8378-c42f46d66f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/subtask_1.zip'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(base_name=\"subtask_1\", format=\"zip\", root_dir=\"llama_guard_few_shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78677e0d-f4dc-400b-a9f2-04f015ff084d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
