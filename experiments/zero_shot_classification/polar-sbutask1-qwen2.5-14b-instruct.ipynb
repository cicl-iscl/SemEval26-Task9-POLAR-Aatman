{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c86ec40d-3544-4ba0-b3fe-2fde218d0190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 74.9 ms, total: 74.9 ms\n",
      "Wall time: 9.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "!pip install peft\n",
    "!pip install evaluate\n",
    "!pip install datasets\n",
    "!pip install transformers==4.51.3\n",
    "!pip install sentencepiece\n",
    "!pip install autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ab9de4-7cfe-4010-866c-0456274eea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: autoawq\n",
      "Version: 0.2.9\n",
      "Summary: AutoAWQ implements the AWQ algorithm for 4-bit quantization with a 2x speedup during inference.\n",
      "Home-page: https://github.com/casper-hansen/AutoAWQ\n",
      "Author: Casper Hansen\n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/conda/lib/python3.12/site-packages\n",
      "Requires: accelerate, datasets, huggingface_hub, tokenizers, torch, transformers, triton, typing_extensions, zstandard\n",
      "Required-by: \n",
      "---\n",
      "Name: transformers\n",
      "Version: 4.51.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /opt/conda/lib/python3.12/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: autoawq, peft\n"
     ]
    }
   ],
   "source": [
    "!pip show autoawq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "969cce2a-746b-4a75-bf00-a0bd68ed21ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 μs, sys: 16 μs, total: 40 μs\n",
      "Wall time: 42.4 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7f81ec4-766b-412f-93fa-f62a1cb5a1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fddeb46-59db-4ab9-9e04-6633ed2eeb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9b59c2b05247668cfec481818c1388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d354c52c84a4635b0d85275451f5932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.58 s, sys: 557 ms, total: 2.14 s\n",
      "Wall time: 3.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_name = \"Qwen/Qwen2.5-14B-Instruct-AWQ\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, dtype=torch.float16, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84e37621-f149-4b63-843e-15eff8432b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGive me a short introduction to large language model.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ad1c051-593a-4c44-89f5-885f46a10e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A large language model is an artificial intelligence system designed to understand and generate human-like text based on the input it receives. These models are typically trained on vast amounts of textual data from the internet and other sources, allowing them to learn patterns in language use, including syntax, semantics, and even contextually appropriate responses. They can perform a wide range of tasks such as answering questions, translating languages, writing stories, summarizing information, and much more. The \"large\" in their name refers to the size of the neural network, which can have billions of parameters, enabling these models to capture complex relationships within language and produce sophisticated outputs.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=512)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d30f4d08-aaa2-44cc-a8c5-f7837e9c2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_folder = \"subtask1/dev/\"\n",
    "output_folder = \"subtask_1\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "932bcb59-87ba-437d-9106-e220430c78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert in social media content analysis, specializing in detecting polarization in online discourse.\n",
    "\n",
    "Polarization is defined as sharp division and hostility between social, political, or identity groups. Polarized content typically:\n",
    "- Creates \"us vs. them\" divisions\n",
    "- Shows extreme hostility toward certain groups\n",
    "- Uses inflammatory or divisive language\n",
    "- Promotes rigid, uncompromising viewpoints\n",
    "- Attacks or demonizes opposing groups\n",
    "- Reinforces group boundaries and antagonism\n",
    "\n",
    "Your task is to classify whether the given text contains polarized content or not.\"\"\"\n",
    "\n",
    "\n",
    "def create_prompt(text):\n",
    "    user_prompt = f\"\"\"Analyze the following social media text and determine if it contains polarized content.\n",
    "\n",
    "Text: \"{text}\"\n",
    "\n",
    "Consider the overall meaning and context, not just individual words. Only classify as polarized if the text clearly reflects attitude polarization with division and hostility.\n",
    "\n",
    "Respond with ONLY one word: \"Yes\" if the text is polarized, or \"No\" if it is not polarized.\"\"\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2c26af1-d3f6-4af9-8146-11ca59e67cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text):\n",
    "    messages = create_prompt(text)\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=10)\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids) :]\n",
    "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = (\n",
    "        tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        .strip()\n",
    "        .lower()\n",
    "    )\n",
    "\n",
    "    return 1 if \"yes\" in response else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a7c61bf-e24e-49e0-9a24-d4a5eb095e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tel:   0%|          | 0/118 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Processing tel: 100%|██████████| 118/118 [00:28<00:00,  4.13it/s]\n",
      "Processing hau: 100%|██████████| 182/182 [00:34<00:00,  5.24it/s]\n",
      "Processing ita: 100%|██████████| 166/166 [00:32<00:00,  5.04it/s]\n",
      "Processing swa: 100%|██████████| 349/349 [01:04<00:00,  5.38it/s]\n",
      "Processing urd: 100%|██████████| 177/177 [00:35<00:00,  4.92it/s]\n",
      "Processing tur: 100%|██████████| 115/115 [00:22<00:00,  5.00it/s]\n",
      "Processing khm: 100%|██████████| 332/332 [01:30<00:00,  3.67it/s]\n",
      "Processing hin: 100%|██████████| 137/137 [00:31<00:00,  4.39it/s]\n",
      "Processing deu: 100%|██████████| 159/159 [00:29<00:00,  5.35it/s]\n",
      "Processing mya: 100%|██████████| 144/144 [00:37<00:00,  3.83it/s]\n",
      "Processing pan: 100%|██████████| 100/100 [00:19<00:00,  5.25it/s]\n",
      "Processing ori: 100%|██████████| 118/118 [00:34<00:00,  3.38it/s]\n",
      "Processing fas: 100%|██████████| 164/164 [00:33<00:00,  4.95it/s]\n",
      "Processing ben: 100%|██████████| 166/166 [00:47<00:00,  3.49it/s]\n",
      "Processing nep: 100%|██████████| 100/100 [00:24<00:00,  4.09it/s]\n",
      "Processing spa: 100%|██████████| 165/165 [00:32<00:00,  5.00it/s]\n",
      "Processing arb: 100%|██████████| 169/169 [00:33<00:00,  5.10it/s]\n",
      "Processing eng: 100%|██████████| 160/160 [00:28<00:00,  5.71it/s]\n",
      "Processing pol: 100%|██████████| 119/119 [00:22<00:00,  5.35it/s]\n",
      "Processing zho: 100%|██████████| 214/214 [00:38<00:00,  5.58it/s]\n",
      "Processing rus: 100%|██████████| 167/167 [00:33<00:00,  4.99it/s]\n",
      "Processing amh: 100%|██████████| 166/166 [00:36<00:00,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 5s, sys: 2min 5s, total: 13min 10s\n",
      "Wall time: 13min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "csv_files = list(Path(dev_folder).glob(\"*.csv\"))\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    lang_code = csv_file.stem\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    predictions = []\n",
    "    for text in tqdm(df[\"text\"].tolist(), desc=f\"Processing {lang_code}\"):\n",
    "        predictions.append(classify_text(text))\n",
    "\n",
    "    output_df = pd.DataFrame({\"id\": df[\"id\"], \"polarization\": predictions})\n",
    "    output_df.to_csv(f\"{output_folder}/pred_{lang_code}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f24a7d7f-446a-4aac-8bb7-cfc8e7f5f84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/subtask_1.zip'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive(\"subtask_1\", \"zip\", \".\", \"subtask_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b44b6a-f406-4182-b06b-66b754f2e92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
